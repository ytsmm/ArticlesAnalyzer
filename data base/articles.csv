10.14529/jsfi140201;Scalability prediction for fundamental performance factors;"Inferring the expected performance for parallel applications is getting harder than ever; applications need to be modeled for restricted or nonexistent systems and performance analysts are required to identify and extrapolate their behavior using only the available resources. Prediction models can be based on detailed knowledge of the application algorithms or on blindly trying to extrapolate measurements from existing architectures and codes. This paper describes the work done to define an intermediate methodology where the combination of (a) the essential knowledge about fundamental factors in parallel codes, and (b) detailed analysis of the application behavior at low core counts on current platforms, guides the modeling efforts to estimate behavior at very large core counts. Our methodology integrates the use of several components like instrumentation package, visualization tools, simulators, analytical models and very high level information from the application running on systems in production to build a performance model."
10.14529/jsfi140202;Predicting the Energy and Power Consumption of Strong and Weak Scaling HPC Applications;Keeping energy costs in budget and operating within available capacities of power distribution and cooling systems is becoming an important requirement for High Performance Computing (HPC) data centers. It is even more important when considering the estimated power requirements for Exascale computing. Power and energy capping are two of emerging techniques aimed towards controlling and efficient budgeting of power and energy consumption within the data center. Implementation of both techniques requires a knowledge of, potentially unknown, power and energy consumption data of the given parallel HPC applications for different numbers of compute servers (nodes).This paper introduces an Adaptive Energy and Power Consumption Prediction (AEPCP) model capable of predicting the power and energy consumption of parallel HPC applications for different number of compute nodes. The suggested model is application specific and describes the behavior of power and energy with respect to the number of utilized compute nodes, taking as an input the available history power/energy data of an application. It provides a generic solution that can be used for each application but it produces an application specific result. The AEPCP model allows for ahead of time power and energy consumption prediction and adapts with each additional execution of the application improving the associated prediction accuracy. The model does not require any application code instrumentation and does not introduce any application performance degradation. Thus it is a high level application energy and power consumption prediction model. The validity and the applicability of the suggested AEPCP model is shown in this paper through the empirical results achieved using two application-benchmarks on the SuperMUC HPC system (the 10th fastest supercomputer in the world, according to Top500 November 2013 rankings) deployed at Leibniz Supercomputing Centre.
10.14529/jsfi140203;SLOWER: A performance model for Exascale computing;A performance framework is introduced to facilitate the development and optimization of extreme-scale abstract execution models and the future systems derived from them. SLOWER defines a six-dimensional design trade-off space based on sources of performance degradation that are invariant across system classes. Exemplar previous generation execution models (e.g., vector) are examined in terms of the SLOWER parameters to illustrate their alternative responses to changing enabling technologies. New technology trends leading to nano-scale and the end of Moore's Law demand future innovations to address these same performance factors. An experimental execution model, ParalleX, is described to postulate one possible advanced abstraction upon which to base next generation hardware and software systems. A detailed examination is presented of how this class of dynamic adaptive execution model addresses SLOWER for advances in efficiency and scalability. To represent the SLOWER trade-off space, a queue model has been developed and is described. A set of simulation experiments spanning ranges of key parameters is presented to expose some initial properties of the SLOWER framework.
10.14529/jsfi140204;Energy Memory and Runtime Tradeoffs for Implementing Collective Communication Operations;Collective operations are among the most important communication operations in shared- and distributed-memory parallel applications. In this paper, we analyze the tradeoffs between energy, memory, and runtime of different algorithms to implement such operations. We show that each existing algorithms have varying behavior and no algorithm exists that is optimal in all three regards. We also show examples where of three different algorithms solving the same problem, each algorithm is best in a different metric. We conclude by posing the challenge to explore the resulting tradeoffs in a more structured manner.
10.14529/jsfi140205;Data Compression for the Exascale Computing Era  Survey;While periodic checkpointing has been an important mechanism for tolerating faults in high-performance computing (HPC) systems, it is cost-prohibitive as the HPC system approaches exascale. Applying compression techniques is one common way to mitigate such burdens by reducing the data size, but they are often found to be less effective for scientific datasets. Traditional lossless compression techniques that look for repeated patterns are ineffective for scientific data in which high-precision data is used and hence common patterns are rare to find. In this paper, we present a comparison of several lossless and lossy data compression algorithms and discuss their methodology under the exascale environment. As data volume increases, we discover an increasing trend of new domain-driven algorithms that exploit the inherent characteristics exhibited in many scientific dataset, such as relatively small changes in data values from one simulation iteration to the next or among neighboring data. In particular, significant data reduction has been observed in lossy compression. This paper also discusses how the errors introduced by lossy compressions are controlled and the tradeoffs with the compression ratio.
10.14529/jsfi140101;Toward Exascale Resilience: 2014 update;Resilience is a major roadblock for HPC executions on future exascale systems. These systems will typically gather millions of CPU cores running up to a billion threads. Projections from current large systems and technology evolution predict errors will happen in exascale systems many times per day. These errors will propagate and generate various kinds of malfunctions, from simple process crashes to result corruptions.The past five years have seen extraordinary technical progress in many domains related to exascale resilience. Several technical options, initially considered inapplicable or unrealistic in the HPC context, have demonstrated surprising successes. Despite this progress, the exascale resilience problem is not solved, and the community is still facing the difficult challenge of ensuring that exascale applications complete and generate correct results while running on unstable systems. Since 2009, many workshops, studies, and reports have improved the definition of the resilience problem and provided refined recommendations. Some projections made during the previous decades and some priorities established from these projections need to be revised. This paper surveys what the community has learned in the past five years and summarizes the research problems still considered critical by the HPC community.
10.14529/jsfi140105;ModelDriven OneSided Factorizations on Multicore Accelerated Systems;Hardware heterogeneity of the HPC platforms is no longer considered unusual but instead have become the most viable way forward towards Exascale.  In fact, the multitude of the heterogeneous resources available to modern computers are designed for different workloads and their efficient use is closely aligned with the specialized role envisaged by their design.  Commonly in order to efficiently use such GPU resources, the workload in question must have a much greater degree of parallelism than workloads often associated with multicore processors (CPUs).  Available GPU variants differ in their internal architecture and, as a result, are capable of handling workloads of varying degrees of complexity and a range of computational patterns.  This vast array of applicable workloads will likely lead to an ever accelerated mixing of multicore-CPUs and GPUs in multi-user environments with the ultimate goal of offering adequate computing facilities for a wide range of scientific and technical workloads.  In the following paper, we present a research prototype that uses a lightweight runtime environment to manage the resource-specific workloads, and to control the dataflow and parallel execution in hybrid systems.  Our lightweight runtime environment uses task superscalar concepts to enable the developer to write serial code while providing parallel execution.  This concept is reminiscent of dataflow and systolic architectures in its conceptualization of a workload as a set of side-effect-free tasks that pass data items whenever the associated work assignment have been completed.  Additionally, our task abstractions and their parametrization enable uniformity in the algorithmic development across all the heterogeneous resources without sacrificing precious compute cycles.  We include performance results for dense linear algebra functions which demonstrate the practicality and effectiveness of our approach that is aptly capable of full utilization of a wide range of accelerator hardware.
10.14529/jsfi140103;Towards a performance portable architecture agnostic implementation strategy for weather and climate models;We propose a software implementation strategy for complex weather and climate models that produces performance portable, architecture agnostic codes. It relies on domain and data structure specific tools that are usable within common model development frameworks -- Fortran today and possibly high-level programming environments like Python in the future. We present the strategy in terms of a refactoring project of the atmospheric model COSMO, where we have rewritten the dynamical core and refactored the remaining Fortran code. The dynamical core is built on top of the domain specific ``Stencil Loop Language'' for stencil computations on structured grids, a generic framework for halo exchange and boundary conditions, as well as a generic communication library that handles data exchange on a distributed memory system. All these tools are implemented in C++ making extensive use of generic programming and template metaprogramming. The refactored code is shown to outperform the current production code and is performance portable to various hybrid CPU-GPU node architectures.
10.14529/jsfi140207;Scalable parallel performance measurement and analysis tools  stateoftheart and future challenges;Current large-scale HPC systems consist of complex configurations with a huge number of potentially heterogeneous components. As the systems get larger, their behavior becomes more and more dynamic and unpredictable because of hard- and software re-configurations due to faultrecovery and power usage optimizations. Deep software hierarchies of large, complex system software and middleware components are required to operate such systems. Therefore, porting, adapting and tuning applications to today's complex systems is a complicated and time-consumingtask. Sophisticated integrated performance measurement, analysis, and optimization capabilities are required to efficiently utilize such systems. This article will summarize the state-of-the-art of scalable and portable parallel performance tools and the challenges these tools are facing on future extreme-scale and big data systems.
10.14529/jsfi140102;RuntimeAware Architectures: A First Approach;In the last few years, the traditional ways to keep the increase of hardware performance at the rate predicted by Moore's Law have vanished. When uni-cores were the norm, hardware design was decoupled from the software stack thanks to a well defined Instruction Set Architecture (ISA). This simple interface allowed developing applications without worrying too much about the underlying hardware, while hardware designers were able to aggressively exploit instruction-level parallelism (ILP) in superscalar processors. With the irruption of multi-cores and parallel applications, this simple interface started to leak. As a consequence, the role of decoupling again applications from the hardware was moved to the runtime system. Efficiently using the underlying hardware from this runtime without exposing its complexities to the application has been the target of very active and prolific research in the last years.Current multi-cores are designed as simple symmetric multiprocessors (SMP) on a chip. However, we believe that this is not enough to overcome all the problems that multi-cores already have to face. It is our position that the runtime has to drive the design of future multi-cores to overcome the restrictions in terms of power, memory, programmability and resilience that multi-cores have. In this paper, we introduce a first approach towards a Runtime-Aware Architecture (RAA), a massively parallel architecture designed from the runtime's perspective.
10.14529/jsfi140106;Exascale Storage Systems  An Analytical Study of Expenses;The computational power and storage capability of supercomputers are growing at a different pace, with storage lagging behind
10.14529/jsfi140104;Communication Complexity of the Fast Multipole Method and its Algebraic Variants;A combination of hierarchical tree-like data structures and data access patterns from fast multipole methods and hierarchical low-rank approximation of linear operators from H-matrix methods appears to form an algorithmic path forward for efficient implementation of many linear algebraic operations of scientific computing at the exascale. The combination provides asymptot- ically optimal computational and communication complexity and applicability to large classes of operators that commonly arise in scientific computing applications. A convergence of the mathe- matical theories of the fast multipole and H-matrix methods has been underway for over a decade. We recap this mathematical unification and describe implementation aspects of a hybrid of these two compelling hierarchical algorithms on hierarchical distributed-shared memory architectures, which are likely to be the first to reach the exascale. We present a new communication complexity estimate for fast multipole methods on such architectures. We also show how the data structures and access patterns of H-matrices for low-rank operators map onto those of fast multipole, leading to an algebraically generalized form of fast multipole that compromises none of its architecturally ideal properties.
10.14529/jsfi140206;Extreme Big Data EBD: Next Generation Big Data Infrastructure Technologies Towards Yottabyte/Year;Our claim is that so-called ``Big Data'' will evolve into a new era with proliferation of data from multiple sources such as massive numbers of sensors whose resolution is increasing exponentially, high-resolution simulations generating huge data results, as well as evolution of social infrastructures that allow for ``opening up of data silos'', i.e., data sources being abundant across the world instead of being confined within an institution, much as how scientific data are being handled in the modern era as a common asset openly accessible within and across disciplines. Such a situation would create the need for not only petabytes to zetabytes of capacity and beyond, but also for extreme scale computing power. Our new project, sponsored under the Japanese JST-CREST program is called ``Extreme Big Data
10.14529/jsfi140305;Codesign of Parallel Numerical Methods for Plasma Physics and Astrophysics;Physically meaningful simulations in plasma physics and astrophysics need powerful hybrid supercomputers equipped with computation accelerators. The development of parallel numerical codes for such supercomputers is a complex scientific problem. In order to solve it the concept of co-design is employed. The co-design is defined as considering the architecture of the supercomputer at all stages of the development of the code. The use of co-design is shown by the example of two physical problems: the interaction of an electron beam with plasma and the collision of galaxies. The resulting speedup and efficiency are shown.
10.14529/jsfi140301;Scientific Big Data Visualization: a coupled tools approach;We designed and implemented a parallel visualisation system for the analysis of large scale time-dependent particle type data. The particular challenge we address is how to analyse a high perfor- mance computation style dataset when a visual representation of the full set is not possible or useful, and one is only interested in finding and inspecting smaller subsets that fulfil certain complex criteria. We used Paraview as the user interface, which is a familiar tool for many HPC users, runs in parallel, and can be conveniently extended. We distributed the data in a supercomputing environment using the Hadoop file system. On top of it, we run Hive or Impala, and implemented a connection between Paraview and them that al- lows us to launch programmable SQL queries in the database di- rectly from within Paraview. The queries return a Paraview-native VTK object that fits directly into the Paraview pipeline. We find good scalability and response times. In the typical supercomputer environment (like the one we used for implementation) the queue and management system make it difficult to keep local data in be- tween sessions, which imposes a bottleneck in the data loading stage. This makes our system most useful when permanently in- stalled on a dedicated cluster.
10.14529/jsfi140302;Research Problems and Opportunities in Memory Systems;The memory system is a fundamental performance and energy bottleneckin almost all computing systems. Recent system design, application,and technology trends that require more capacity, bandwidth,efficiency, and predictability out of the memory system make it aneven more important system bottleneck. At the same time, DRAMtechnology is experiencing difficult {\em technology scaling}challenges that make the maintenance and enhancement of its capacity,energy-efficiency, and reliability significantly more costly withconventional techniques.In this article, after describing the demands and challenges faced bythe memory system, we examine some promising research and designdirections to overcome challenges posed by memoryscaling. Specifically, we describe three major {\em new} researchchallenges and solution directions: 1) enabling new DRAMarchitectures, functions, interfaces, and better integration of theDRAM and the rest of the system (an approach we call {\em system-DRAM                          co-design}), 2) designing a memory system that employs emergingnon-volatile memory technologies and takes advantage of multipledifferent technologies (i.e., {\em hybrid memory systems}), 3)providing predictable performance and QoS to applications sharing thememory system (i.e., {\em QoS-aware memory systems}). We also brieflydescribe our ongoing related work in combating scaling challenges ofNAND flash memory. 
10.14529/jsfi140304;Heterogeneous parallel computing: from clusters of workstations to hierarchical hybrid platforms;The paper overviews the state of the art in design and implementation of data parallelscientic applications on heterogeneous platforms. It covers both traditional approaches originallydesigned for clusters of heterogeneous workstations and the most recent methods developed in thecontext of modern multicore and multi-accelerator heterogeneous platforms.
10.14529/jsfi140303;Early evaluation of direct largescale InfiniBand networks with adaptive routing;We assess the problem of choosing optimal direct topology for InfiniBand networks in terms of performance. Newest topologies like Dragonfly, Flattened butterfly and Slim Fly are considered, as well as standard Tori and Hypercubes.We consider some reasonable extensions to InfiniBand hardware which could be implemented by vendors easily and may allow reasonable routing algorithms for such topologies. A number of routing algorithms are proposed and compared for various traffic patterns. Mapping algorithms for Dragonfly and Flattened Butterfly are proposed. Based on this research it has been decided to use Flattened Butterfly topology for system #22 in November 2014 Top 500 list.
10.14529/jsfi150202;Acceleration of MPI mechanisms for sustainable HPC applications;Ultrascale computing systems are meant to reach a growth of two or three orders of magnitude of today computing systems. However, to achieve the performances required, we will need to design and implement more sustainable solutionsfor ultra-scale computing systems, understanding sustainability in a holistic manner to address challenges as economy-of-scale, agile elastic scalability, heterogeneity, programmability, fault resilience, energy efficiency, and scalable storage. Some of those solutions could be provided into MPI, but other should be devised as higher level concepts, less generalists, but adapted to applicative domains, possibly as programming patterns or or libraries. In this paper, we show some proposals to extend MPI trying to cover major domains that are relevant towards sustainability: MPI programming optimizations and programming models, resilience, data management, and their usage from applications.
10.14529/jsfi150104;Neohetergeneous Programming and Parallelized Optimization of a Human Genome Resequencing Analysis Software Pipeline on TH2 Supercomputer;The growing velocity of biological big data is way beyond Moore's Law of compute power growth. The amount of genomic data has been explosively accumulating, which calls for an enormous amount of computing power, while current computation methods cannot scale out with the data explosion. In this paper, we try to utilize huge computing resources to solve thebig dataproblems of genome processing on TH-2 supercomputer. TH-2 supercomputer adopts neo-heterogeneous architecture and owns 16,000 compute nodes: 32000 Intel Xeon CPUs + 48000 Xeon Phi MICs. The heterogeneity, scalability, and parallel efficiency pose great challenges forthe deployment of the genomeanalysis software pipeline on TH-2. Runtime profiling shows that SOAP3-dp and SOAPsnp are the most time-consuming parts (up to 70% of total runtime) in the whole pipeline, which need parallelized optimization deeply and large-scale deployment. To address this issue, we first designa series of new parallel algorithms for SOAP3-dp and SOAPsnp, respectively, to eliminatethe spatial-temporal redundancy. Then we propose a CPU/MIC collaboratedparallel computing method in one node to fully fill the CPU/MIC time slots. We also propose a series ofscalable parallel algorithms and large scaleprogramming methods to reduce the amount of communications between different nodes. Moreover, we deploy and evaluate our works on the TH-2 supercomputer in different scales. At the most large scale, the whole process takes 8.37 hours using 8192 nodes to finish the analysis of a 300TB dataset of whole genome sequences from 2,000 human beings, which can take as long as 8 months on a commodity server. The speedup is about 700x.
10.14529/jsfi150205;Energyefficient Algorithms for Ultrascale Systems;The chances to reach Exascale or Ultrascale Computing are strongly connected with the problem of the energy consumption for processing applications. For physical as well as economical reasons, the energy consumption has to be reduced significantly to make Ultrascale Computing possible. The research efforts towards energy-saving mechanisms of the hardware has already led to energy-aware hardware systems available today. However, hardware mechanisms can only obtain an energy reduction if software can exploit them such that energy-efficient computing actually results. In the software area, there also exists a multitude of research approaches towards energy saving. These research approaches and results are often isolated either on the system software level or the application organization level, reflecting the expertise of the corresponding research group. The challenge of reducing the energy consumption dramatically to make Ultrascale Computing possible are so ambitions that a concerted action combining all these software levels and research efforts seems reasonable. In this article, we demonstrate the current research efforts and results related to energy in the diverse areas of software. Moreover, we conclude with open problems and questions concerning energy-related techniques with an emphasis on the application algorithmic side.
10.14529/jsfi150102;Applications for ultrascale computing;Studies of complex physical and engineering systems, represented by multi-scale and multi-physics computer simulations have an increasing demand for computing power, especially when the simulations of realistic problems are considered. This demand is driven by the increasing size and complexity of the studied systems or the time constraints. Ultrascale computing systems offer a possible solution to this problem. Future ultrascale systems will be large-scale complex computing systems combining technologies from high performance computing, distributed systems, big data, and cloud computing. Thus, the challenge of developing and programming complex algorithms on these systems is twofold. Firstly, the complex algorithms have to be either developed from scratch, or redesigned in order to yield high performance, while retaining correct functional behaviour. Secondly, ultrascale computing systems impose a number of non-functional cross-cutting concerns, such as fault tolerance or energy consumption, which can significantly impact the deployment of applications on large complex systems. This article discusses the state-of-the-art of programming for current and future large scale systems with an emphasis on complex applications. We derive a number of programming and execution support requirements by studying several computing applications that the authors are currently developing and discuss their potential and necessary upgrades for ultrascale execution.
10.14529/jsfi150201;Exascale Machines Require New Programming Paradigms and Runtimes;Extreme scale parallel computing systems will have tens of thousands of optionally accelerator-equiped nodes with hundreds of cores each, as well as deep memory hierarchies and complex interconnect topologies. Such Exascale systems will provide hardware parallelism at multiple levels and will be energy constrained. Their extreme scale and the rapidly deteriorating reliablity of their hardware components means that Exascale systems will exhibit low mean-time-between-failure values. Furthermore, existing programming models already require heroic programming and optimisation efforts to achieve high efficiency on current supercomputers. Invariably, these efforts are platform-specific and non-portable. In this paper we will explore the shortcomings of existing programming models and runtime systems for large scale computing systems. We then propose and discuss important features of programming paradigms and runtime system to deal with large scale computing systems with a special focus on data-intensive applications and resilience.Finally, we also discuss code sustainability issues and propose several software metrics that are of paramount importance for code development for large scale computing systems.
10.14529/jsfi150204;Energy Measurement Tools for Ultrascale Computing: A Survey;With energy efficiency one of the main challenges on the way towards ultrascale systems, there is great need for access to high-quality energy consumption data. Such data would enable researchers and designers to pinpoint energy inefficiencies at all levels of the computing stack, from whole nodes down to critical regions of code. However, measurement capabilities are often missing, and significantly differ between platforms where they exist. A standard is yet  to be established. To that end, this paper attempts an extensive survey of energy measurement tools currently available at both the hardware and software level, comparing their features with respect to energy monitoring.
10.14529/jsfi150203;Resilience within Ultrascale Computing System: Challenges and Opportunities from Nesus Project;Ultrascale computing is a new computing paradigm that comes naturally from the necessity of computing systems that should be able to handle massive data in possibly very large scale  distributed systems, enabling new forms of applications that can serve a very large amount of  users and in a timely manner that we have never experienced before. However, besides the benefits,  ultrascale computing systems do not come without challenges. One of the challenges is the resilience  of ultrascale computing systems. Although resilience is already an established field in system  science and many methodologies and approaches are available to deal with it, the unprecedented  scales of computing, of the massive data to be managed, new network technologies, and drastically  new forms of massive scale applications bring new challenges that need to be addressed. This paper  reviews the challenges and approaches of resilience in ultrascale computing systems from multiple  perspectives involving and addressing the resilience aspects of hardware-software co-design for  ultrascale systems, resilience against (security) attacks, new approaches and methodologies to  resilience in ultrascale systems, applications and case studies.
10.14529/jsfi150206;Energy Efficiency for Ultrascale Systems: Challenges and Trends from Nesus Project;Energy consumption is one of the main limiting factors for designing and deploying ultrascale systems. Therefore, this paper presents challenges and trends associated with energy efficiency for ultrascale systems based on current activities of the working group on Energy Efficiency in the European COST Action Nesus IC1305. The analysis contains major areas that are related to studies of energy efficiency in ultrascale systems: heterogeneous and low power hardware architectures, power monitoring at large scale, modeling and simulation of ultrascale systems, energy-aware scheduling and resource management, and energy-efficient application design.
10.14529/jsfi150103;Dense Matrix Computations on NUMA Architectures with DistanceAware Work Stealing;We employ the dynamic runtime system OmpSs to decrease the overhead of data motion in the now ubiquitous non-uniform memory access (NUMA) high concurrency environment of multicore processors. The dense numerical linear algebra algorithms of Cholesky factorization and symmetric matrix inversion are employed as representative benchmarks. Work stealing occurs within an innovative NUMA-aware scheduling policy to reduce data movement between NUMA nodes. The overall approach achieves separation of concerns by abstracting the complexity of the hardware from the end users so that high productivity can be achieved. Performance results on a large NUMA system outperform the state-of-the-art existing implementations up to a two fold speedup for the Cholesky factorization, as well as the symmetric matrix inversion, while the OmpSs-enabled code maintains strong similarity to its original sequential version.
10.14529/jsfi150307;A Case for Embedded FPGAbased SoCs in EnergyEfficient Acceleration of Graph Problems;Sparse graph problems are notoriously hard to accelerate on conventional platforms due to irregular memory access patterns resulting in underutilization of memory bandwidth. These bottlenecks on traditional x86-based systems mean that sparse graph problems scale very poorly, both in terms of performance and power efficiency. A cluster of embedded SoCs (systems-on-chip) with closely-coupled FPGA accelerators can support distributed memory accesses with better matched low-power processing. We first conduct preliminary experiments across a range of COTS (commercial off-the-shelf) embedded SoCs to establish promise for energy-efficiency acceleration of sparse problems. We select the Xilinx Zynq SoC with FPGA accelerators to construct a prototype 32-node Beowulf cluster. We develop specialized MPI routines and memory DMA offload engines to support irregular communication efficiently. In this setup, we use the ARM processor as a data marshaller for local DMA traffic as well as remote MPI traffic while the FPGA may be used as a programmable accelerator. Across a set of benchmark graphs, we show that 32-node embedded SoC cluster can exceed the energy efficiency of an Intel E5-2407 by as much as 1.7 at a total graph processing capacity of 91-95 MTEPS for graphs as large as 32 million nodes and edges. 
10.14529/jsfi150305;An Autonomic Performance Environment for Exascale;"Exascale systems will require  new approaches to performance observation, analysis, and runtime decision-making to optimize for performance and efficiency. The standard ""first-person"" model, in which multiple operating system processes and threads observe themselves and record first-person performance profiles or traces for offline analysis, is not adequate to observe and capture interactions at shared resources in highly concurrent, dynamic systems. Further, it does not support mechanisms for runtime adaptation. Our approach, called APEX (Autonomic Performance Environment for eXascale), provides mechanisms for sharing information among the layers of the software stack, including hardware, operating and runtime systems, and application code, both new and legacy. The performance measurement components share information  across layers, merging first-person data sets with information collected by  third-person tools observing shared hardware and software states at  node- and global-levels. Critically, APEX provides a policy engine designed to guide runtime adaptation mechanisms to make algorithmic changes, re-allocate resources, or change scheduling rules when appropriate conditions occur."
10.14529/jsfi150304;The LCSC cluster: Optimizing power efficiency to become the greenest supercomputer in the world in the Green500 list of November 2014;The L-CSC (Lattice Computer for Scientific Computing) is a general purpose compute cluster built with commodity hardware installed at GSI. Its main operational purpose is Lattice QCD (LQCD) calculations for physics simulations. Quantum Chromo Dynamics (QCD) is the physical theory describing the strong force, one of the four known fundamental interactions in the universe. L-CSC leverages a multi-GPU design accommodating the huge demand of LQCD for memory bandwidth. In recent years, heterogeneous clusters with accelerators such as GPUs have become more and more powerful while supercomputers in general have shown enormous increases in power consumption making electricity costs and cooling a significant factor in the total cost of ownership. Using mainly GPUs for processing, L-CSC is very power-efficient, and its architecture was optimized to provide the greatest possible power efficiency. This paper presents the cluster design as well as optimizations to improve the power efficiency. It examines the power measurements performed for the Green500 list of the most power-efficient supercomputers in the world which led to the number 1 position as the greenest supercomputer in November 2014.
10.14529/jsfi150101;AlgoWiki: an Open Encyclopedia of Parallel Algorithmic Features;The main goal of this project is to formalize the mapping of algorithms onto the architecture of parallel computing systems. The basic idea is that features of algorithms are independent of any computing system. A detailed description of a given algorithm with a special emphasis on its parallel properties is made once, and after that it can be used repeatedly for various implementations of the algorithm on different computing platforms. Machine-dependent, part of this work is devoted to describing features of algorithms implementation for different parallel architectures. The proposed description of algorithms includes many non-trivial features such as: parallel algorithm complexity, resource of parallelism and its properties, features of the informational graph, computational cost of algorithms, data locality analysis as well as analysis of scalability potential, and many others. Descriptions of algorithms form the basis of AlgoWiki, which allows for collaboration with the computing community in order to produce different implementations and achieve improvement. Project website: http://algowiki-project.org/en/
10.14529/jsfi150401;Live Programming in Scientific Simulation;"We demonstrate that a live-programming environment can be used to harness and add run-time interactivity to scientific simulation codes. Through a set of examples using a Particle-In-Cell (PIC) simulation framework we show how the real-time, human-in-the-loop interactivity of live programming can be incorporated into a traditional, ""offline"", development workflow. We discuss how live programming tools and techniques can be productively integrated into the existing HPC landscape to increase productivity and enhance exploration and discovery."
10.14529/jsfi150303;Performance Assessment of InfiniBand HPC Cloud Instances on Intel Haswell and Intel Sandy Bridge Architectures;This paper aims to establish a performance baseline of a HPC installation of OpenStack. We created InfiniCloud - a distributed High Performance Cloud hosted on remote nodes of InfiniCortex. InfiniCloud compute nodes use high performance Intel (R) Haswell and Sandy Bridge CPUs, SSD storage and 64-256GB RAM. All computational resources are connected by high performance IB interconnects and are capable of trans-continental IB communication using Obsidian Longbow range extenders.We benchmark the performance of our test-beds using micro-benchmarks for TCP bandwidth, IB bandwidth and latency, file creation performance, MPI collectives and Linpack. This paper compares different CPU generations across virtual and bare-metal environments.The results show modest improvements in TCP and IB bandwidth and latency on Haswell
10.14529/jsfi150302;InfiniCloud: Leveraging the Global InfiniCortex Fabric and OpenStack Cloud for Borderless High Performance Computing of Genomic Data;At the Supercomputing Frontiers Conference in Singapore in 2015, A*CRC (Singapore) and NCI (Canberra, Australia) presented InfiniCloud, a geographically distributed, high performance InfiniBand HPC Cloud which aims to enable borderless processing of genomic data as part of the InfiniCortex project. This paper provides a high-level technical overview of the architecture of InfiniCloud and how it can be used for high performance computation of genomic data in geographically distant sites by encapsulation of workflows/applications in Virtual Machines (VM) coupled with on-the-fly configuration of clusters and high speed transfer of data via long range InfiniBand. 
10.14529/jsfi150306;Visualization for Exascale: Portable Performance is Critical;Researchers face a daunting task to provide scientific visualization capabilities for exascale computing. Of the many fundamental changes we are seeing in HPC systems, one of the most profound is a reliance on new processor types optimized for execution bandwidth over latency hiding. Multiple vendors create such accelerator processors, each with significantly different features and performance characteristics. To address these visualization needs across multiple platforms, we are embracing the use of data parallel primitives that encapsulate highly efficient parallel algorithms that can be used as building blocks for conglomerate visualization algorithms. We can achieve performance portability by optimizing this small set of data parallel primitives whose tuning conveys to the conglomerates.
10.14529/jsfi150301;Data Exploration at the Exascale;"In situ processing - i.e., coupling visualization routines to a simulation code to generate images in real-time - is predicted to be the dominant form for visualization on upcoming supercomputers. Unfortunately, traditional in situ techniques are largely incongruent with exploratory visualization, which is an important activity to enable understanding of simulation data. In re- sponse, a new paradigm is emerging: data is transformed and massively reduced in situ and then the resulting form is explored post hoc. The fundamental tension in this approach is between the extent of the data reduction and the loss in integrity in the resulting data. However, new oppor- tunities, in terms of increased access to data, may blunt this tension and allow for both sufficient data reduction and also more accurate analysis. With this paper, we describe the trends behind ""data exploration at the exascale"" and also summarize some recent results that confirmed that this new paradigm can produce superior results compared to the traditional one. "
10.14529/jsfi150402;Creating interconnect topologies by algorithmic edge removal: MOD and SMOD graphs;We introduce a method of constructing classes of graphs by algorithmic removal of entire groups of edges. Our approach to creating new classes of graphs is to focus entirely on the structure and properties of the adjacency matrix. At an initialisation step of the algorithm we start with a complete (fully connected) graph. In Part I we present MOD and arrested MOD graphs resulting from removal of square blocks of edges at each iteration and substitution of removed blocks with a diagonal matrix with one extra pivotal element along the main diagonal. The MOD graphs possess unique and useful properties. All important graph measures are easily expressed in analytical form and are presented in the paper. Several important properties of MOD graphs compare very favourably with graphs representing common interconnect topologies: hypercube, 3D and 5D tori, TOFU and dragony. This lead us to consider MOD and arrested MOD graphs as interesting candidats for eective supercomputer interconnects.In Part II, at each iterative step we successively remove triangular shapes from adjacency matrix. This iterative process leads to the nal matrix which has two Sierpinski gaskets aligned along the main diagonal. It will be shown below, that this new class of graphs is not a Sierpinski graph, since it is the adjacency matrix which has a structure of a Sierpinski gasket, and not a graph described by this matrix. We call this new class of graphs Sierpinski-Michalewicz-Or lowski-Deng (SMOD) graphs. The most remarkable property of the SMOD class of graphs, is that irrespective of the graph order, the diameter is constant and equals 2. The size of the graph, or the total number of edges, is about 10% of the size of a complete graph of the same order. We analyse important graph theoretic characte-ristics related to the topology such as diameter as a function of graph order, size, mean path length, ratio of the graph size to the size of a complete graph of the same order, and some spectral properties.Keywords: supercomputer interconnects, big data, exascale computing, graph theory,topology of graphs, classes of graphs, graph generation.
10.14529/jsfi150403;MultiScale Supercomputing of Large Molecular Aggregates: A Case Study of the LightHarvesting Photosynthetic Center;Numerical solution of the quantum mechanical Schr?dinger equation is required to model electronic excitations in the light-harvesting photosynthetic complexes composed of up to millions of atoms. We demonstrate that the modern supercomputers can be used to treat electronic structure calculations in such large molecular aggregates if proper multi-scale massive-parallel approaches are applied. We show that the three-level parallelization scheme based on the novel numerical algorithms assuming fragmentation of a light-harvesting complex allows us to reduce considerably the high scaling of ab initio quantum chemistry methods. More specifically we applied the time-dependent density functional theory based upon the fragment molecular orbital presentation (FMO-TDDFT) implemented at the modern supercomputers to obtain a realistic estimate of the electronic excitation in the complex. The application shows a good overall scaling.  
10.14529/jsfi150404;Parallel software platform INMOST: a framework for numerical modeling;The INMOST mathematical modeling toolkit helps a user to formulate and solve a problem of partial differential equations on general meshes in parallel. The current work covers: data structure description for efficient distributed unstructured mesh representation, interrelation of mesh elements with maximal flexibility of supported types of the mesh, treatment of ghost cells and distribution of mesh data for parallel execution, flexible templates for the implementation of numerical schemes, convenient framework for parallel linear systems assembly and solution. We also present aspects of the implementation and a simple example of application of INMOST to the solution of anisotropic diffusion problem. On this example we demonstrate the application of INMOST for all the stages of numerical modeling: construction of the distributed mesh, assignment of the problem data to the elements, problem discretization on local domain, solution of linear system in parallel. INMOST is a newly developed, flexible and efficient numerical analysis framework that provides scientists the infrastructure for designing highly scalable high performance applications for mathematical modeling.
10.14529/jsfi160101;NRMPI: A Nonstop and Fault Resilient MPI Supporting Programmer Defined Data Backup and Restore for Escale Super Computing Systems;Fault resilience has became a major issue for HPC systems, particularly, in the perspective of future E-scale systems, which will consist of millions of CPU cores and other components. MPI-level fault tolerant constructs, such as ULFM, are being proposed to support software level fault tolerance. However, there are few systematic evaluations by application programmers using benchmarks or pseudo applications. This paper proposes NR-MPI, a \emph{N}on-stop and Fault \emph{R}esilient \emph{MPI}, supporting programmer defined data backup and restore. To help programmers write fault tolerant programs, NR-MPI provides a set of friendly programming interfaces and a state transition diagram for data backup and restore. This paper focuses on design, implementation and evaluation of NR-MPI. Specifically,this paper puts emphases on failure detection in MPI library, friendly programming interface extending for NR-MPI and examples of fault tolerant programs based NR-MPI. Furthermore, to support failure recovery of applications, NR-MPI implements data backup interfaces based on double in-memory checkpoint/restart. We conduct experiments with both NPB benchmarks and Sweep3D on TH supercomputer in NSCC-TJ. Experimental results show that NR-MPI based fault tolerant programs can recover from failures online without restarting, and the overhead is small even for applications with tens of thousands of cores.
10.14529/jsfi150405;Parallel Programming Models for Dense Linear Algebra on Heterogeneous Systems;We present a review of the current best practices in parallel programming models for dense linear algebra (DLA) on heterogeneous architectures. We consider multicore CPUs, stand alone manycore coprocessors, GPUs, and combinations of these. Of interest is the evolution of the programming models for DLA libraries { in particular, the evolution from the popular LAPACK and ScaLAPACK libraries to their modernized counterparts PLASMA (for multicore CPUs) and MAGMA (for heterogeneous architectures), as well as other programming models and libraries.Besides providing insights into the programming techniques of the libraries considered, we outline our view of the current strengths and weaknesses of their programming models { especially in regards to hardware trends and ease of programming high-performance numerical software that current applications need { in order to motivate work and future directions for the next generation of parallel programming models for high-performance linear algebra libraries on heterogeneous systems.
10.14529/jsfi160201;A Dynamic Congestion Management System for InfiniBand Networks;While the InfiniBand link-by-link flow control helps avoid packet loss, it unfortunately causes the effects of congestion to spread through a network. Flows whose paths do not even pass through congested ports could suffer from reduced throughput. We propose a Dynamic Congestion Management System (DCMS) to address this problem. Without per-flow information, the DCMS leverages performance counters of switch ports to detect onset of congestion, and determines whether-or-not victim flows are present. The DCMS then takes actions to cause an aggressive reduction in the sending rates of congestion-causing (contributor) flows if victim flows are present. On the other hand, in the absence of victim flows, the DCMS allows the contributor flows to maintain high sending rates and finish as quickly as possible.Our results show that dynamic congestion management can enable a network to serve both contributor flows and victim flows effectively. The DCMS solution operates within the constraints of the InfiniBand Standard.
10.14529/jsfi160202;ManyCore Approaches to Combinatorial Problems: case of the Langford Problem;As observed from the last TOP500 list - November 2015 -, GPUs-accelerated clusters emerge as clear evidence. But exploiting such architectures for combinatorial problem resolution remains a challenge. In this context, this paper focuses on the resolution of an academic combinatorial problem, known as Langford pairing problem, which can be solved using several approaches. We first focus on a general solving scheme based on CSP (Constraint Satisfaction Problem) formalism and backtrack called the Miller algorithm. This method enables us to compute instances up to L(2,21) using both CPU and GPU computational power with load balancing.As dedicated algorithms may still have better computation efficiency we took advantage of the Godfrey algebraic method to solve the Langford problem and implemented it using our multiGPU approach. This allowed us to recompute the last open instances, L(2, 27) and L(2, 28), respectively in less than 2 days and 23 days using best-effort computation on the ROMEO supercomputer with up to 500,000 GPU cores.
10.14529/jsfi160203;A Radical Approach to Computation with Real Numbers;"If we are willing to give up compatibility with IEEE 754 floats and design a number format with goals appropriate for 2016, we can achieve several goals simultaneously: Extremely high energy efficiency and information-per-bit, no penalty for decimal operations instead of binary, rigorous bounds on answers without the overly pessimistic bounds produced by interval methods, and unprecedented high speed up to some precision. This approach extends the ideas of unum arithmetic introduced two years ago by breaking completely from the IEEE float-type format, resulting in fixed bit size values, fixed execution time, no exception values or ""gradual underflow"" issues, no wasted bit patterns, and no redundant representations (like ""negative zero""). As an example of the power of this format, a difficult 12-dimensional nonlinear robotic kinematics problem that has defied solvers to date is quickly solvable with absolute bounds. Also unlike interval methods, it becomes possible to operate on arbitrary disconnected subsets of the real number line with the same speed as operating on a simple bound."
10.14529/jsfi160204;InfiniCloud 20: distributing High Performance Computing across continents;InfiniCloud 2.0 is World's first native InfiniBand High Performance Cloud distributed across four continents, spanning Asia, Australia, Europe and North America. The project provides researchers with instant access to computational, storage and network resources distributed around the globe. These resources are then used to build a geographically distributed, virtual supercomputer, complete with globally-accessible parallel file system and job scheduling.This paper describes high level design and the implementation details of InfiniCloud 2.0. A gene sequencing pipeline as well as plasma physics simulation code are used to demonstrate system's capabilities.
10.14529/jsfi160205;Making LargeScale Systems Observable  Another Inescapable Step Towards Exascale;The effective mastering of extremely parallel HPC system is impossible without deep understanding of all internal processes and behavior of the whole diversity of the components: computing processors and nodes, memory usage, interconnect, storage, whole software stack, cooling and so forth in detail. There are numerous visualization tools that provide information on certain components and system as a whole, but most of them have severe issues that limit appliance in real life, thus becoming inacceptable for the future system scales.Predefined monitoring systems and data sources, lack of dynamic on-the-fly reconfiguration, inflexible visualization and screening options are among the most popular issues.The proposed approach to monitoring data processing resolves the majority of known problems, providing a scalable and flexible solution based on any available monitoring systems and other data sources. The approach implementation is successfully used in every-day practice of the largest in Russia supercomputer center of Moscow State University.
10.14529/jsfi160102;Reconfigurable computer systems: from the first FPGAs towards liquid cooling systems;The paper covers the history of development of design technologies of reconfigurable computer systems based on FPGAs of various families. Five generations of reconfigurable computer systems with high placement density, designed on the base of various FPGA families, from Xilinx Virtex-E to modern Virtex UltraScale, are described. The last achievements in the domain of design of energetic effective reconfigurable computer systems with high real performance are presented. One of such achievements is the developed liquid cooling system for Virtex UltraScale FPGAs. It provides independent circulation of the cooling liquid in the 3U computational module with the 19'' height for cooling of 96-128 FPGA chips that in total generate 9.6-12.8 kWatt of heat. The distinctive features of the designed immersion liquid cooling system are high cooling efficiency with power reserve for the designed perspective FPGA families, resistance to leaks and their consequences, and compatibility with traditional water cooling systems based on industrial chillers.
10.14529/jsfi160103;Supercomputer technologies in tomographic imaging applications;Currently, tomographic imaging is widely used in medical and industrial non-destructive testing applications. X-ray tomography is the prevalent imaging technology. Modern medical X-ray CT scanners provide up to 1 mm spatial resolution. The disadvantage of X-ray tomography is that it cannot be used for regular medical examinations. Early breast cancer diagnosis is one of the most pressing issues in modern healthcare. Ultrasound tomography devices are being developed in USA, Germany and Russia to address this problem. One of the main challenges in ultrasound tomographic imaging is the development of efficient algorithms for solving inverse problems of wave tomography, which are nonlinear three-dimensional coefficient inverse problems for a hyperbolic differential equation. Solving such computationally-expensive problems requires the use of supercomputers.
10.14529/jsfi160104;Server Level Liquid Cooling: Do Higher System Temperatures Improve Energy Effciency;Liquid cooling is now a mainstream approach to boost energy effciency for high performance computing systems. Higher coolant temperature is usually considered as an advantage, since it allows heat reuse/recuperation and simplifies datacenter infrastructure by eliminating the need of chiller machine. However, the use of hot coolant imposes high requirements for cooling equipment. A promising approach is to utilize coldplates with channel structure and liquid circulation for heat removal from semiconductor components. We have designed a coldplate with low heat-resistance that ensures effective cooling with only 2030° temperature difference between coolant and electronic parts of a server. Under stress-test conditions the coolant temperature was up to 65 °C while server operation was undisturbed. We also studied power effciency (expressed in floating point operations per watt) dependence on the coolant temperature (19-65 °C) on theindividualserverlevel (based on Intel Grantley platform with dual Intel Xeon E5-2697v3 processors). Ð ÑThe power performance ratio shows moderate (=10%) ef?ciency drop from 19 to 65°C due to increase of leak age current in chipset components and reduction of processor frequency resulted into proportional reduction of DGEMM benchmark performance. It must be taken into account by datacenter designers, that the amount of recuperated energy from 65 °C should be at least=10% to justify the choice of high temperature coolant solution.
10.14529/jsfi160105;Data Compression for Climate Data;The different rates of increase for computational power and storage capabilities of supercomputers turn data storage into a technical and economical problem. Because storage capabilities are lagging behind, investments and operational costs for storage systems have increased to keep up with the supercomputers' I/O requirements. One promising approach is to reduce the amount of data that is stored. In this paper, we take a look at the impact of compression on performance and costs of high performance systems. To this end, we analyze the applicability of compression on all layers of the I/O stack, that is, main memory, network and storage. Based on the Mistral system of the German Climate Computing Center (Deutsches Klimarechenzentrum, DKRZ), we illustrate potential performance improvements and cost savings. Making use of compression on a large scale can decrease investments and operational costs by 50% without negatively impacting performance. Additionally, we present ongoing work for supporting enhanced adaptive compression in the parallel distributed file system Lustre and application-specific compression.
10.14529/jsfi160206;Application of CUDA technology to calculation of ground states of fewbody nuclei by Feynman's continual integrals method;The possibility of application of modern parallel computing solutions to speed up the calculations of ground states of few-body nuclei by Feynman's continual integrals method has been investigated. These calculations may sometimes require large computational time, particularly in the case of systems with many degrees of freedom. This paper presents the results of application of general-purpose computing on graphics processing units (GPGPU). The energy and the square modulus of the wave function of the ground states of several few-body nuclei have been calculated using NVIDIA CUDA technology. The results show that the use of GPGPU significantly increases the speed of calculations.
10.14529/jsfi160301;Hybrid CPU + Xeon Phi implementation of the ParticleinCell method for plasma simulation;This paper presents experimental results of Particle-in-Cell plasma simulation on a hybrid system with CPUs and Intel Xeon Phi coprocessors. We consider simulation of two relevant laser-driven particle acceleration regimes using the Particle-in-Cell code PICADOR. On a node of a cluster with 2 CPUs and 2 Xeon Phi coprocessors the hybrid CPU + Xeon Phi configuration allows to fully utilize the computational resources of the node. It outperforms both CPU-only and Xeon Phi-only configurations with the speedups between 1.36 x and 1.68 x.
10.14529/jsfi160302;Easy Access to HPC Resources through the Application GUI;The computing environment at the King Abdullah University of Science and Technology (KAUST) is growing in size and complexity. KAUST hosts the tenth fastest supercomputer in the world (Shaheen II) and several HPC clusters. Researchers can be inhibited by the complexity, as they need to learn new languages and execute many tasks in order to access the HPC clusters and the supercomputer. In order to simplify the access, we have developed an interface between the applications and the clusters and supercomputer that automates the transfer of input data and job submission and also the retrieval of results to the researcher's local workstation. The innovation is that the user now submits his jobs from within the application GUI on his workstation, and does not have to directly log into the clusters or supercomputer anymore. This article details the solution and its benefits to the researchers.
10.14529/jsfi160303;Predicting I/O Performance in HPC Using Artificial Neural Networks;The prediction of file access times is an important part for the modeling of supercomputer's storage systems. These models can be used to develop analysis tools which support the users to integrate efficient I/O behavior.In this paper, we analyze and predict the access times of a Lustre file system from the client perspective. Therefore, we measure file access times in various test series and developed different models for predicting access times.  The evaluation shows that in models utilizing artificial neural networks the average prediciton error is about 30% smaller than in linear models. A phenomenon in the distribution of file access times is of particular interest: File accesses with identical parameters show several typical access times.The typical access times usually differ by orders of magnitude and can be explained with a different processing of the file accesses in the storage system - an alternative I/O path. We investigate a method to automatically determine the alternative I/O path and quantify the significance of knowledge about the internal processing. It is shown that the prediction error is improved significantly with this approach.
10.14529/jsfi160304;Analyzing Data Properties using Statistical Sampling - Illustrated on Scientific File Formats;Understanding the characteristics of data stored in data centers helps computer scientists in identifying the most suitable storage infrastructure to deal with these workloads. For example, knowing the relevance of file formats allows optimizing the relevant formats but also helps in a procurement to define benchmarks that cover these formats. Existing studies that investigate performance improvements and techniques for data reduction such as deduplication and compression operate on a subset of data. Some of those studies claim the selected data is representative and scale their result to the scale of the data center. One hurdle of running novel schemes on the complete data is the vast amount of data stored and, thus, the resources required to analyze the complete data set. Even if this would be feasible, the costs for running many of those experiments must be justified.This paper investigates stochastic sampling methods to compute and analyze quantities of interest on file numbers but also on the occupied storage space. It will be demonstrated that on our production system, scanning 1% of files and data volume is sufficient to deduct conclusions. This speeds up the analysis process and reduces costs of such studies significantly.
10.14529/jsfi160305;Spectral Domain Decomposition Using Local Fourier Basis: Application to Ultrasound Simulation on a Cluster of GPUs;The simulation of ultrasound wave propagation through biological tissue has a wide range of practical applications. However, large grid sizes are generally needed to capture the phenomena of interest. Here, a novel approach to reduce the computational complexity is presented. The model uses an accelerated k-space pseudospectral method which enables more than one hundred GPUs to be exploited to solve problems with more than 3*10^9 grid points. The classic communication bottleneck of Fourier spectral methods, all-to-all global data exchange, is overcome by the application of domain decomposition using local Fourier basis. Compared to global domain decomposition, for a grid size of 1536 x 1024 x 2048, this reduces the simulation time by a factor of 7.5 and the simulation cost by a factor of 3.8.
10.14529/jsfi160306;HCA aware Parallel Communication Library: A feasibility study for offloading MPI requirements;Message Passing Interface (MPI) is a standardized message passing system, independent of underlying network, and the most widely used parallel programming paradigm. The communication library should make full use of the Host Channel Adapter (HCA) characteristics to maximize performance of the HPC cluster. The communication library may not able to take full advantage of the underlying network adapter, if the library is made generalized. This can have a significant impact on the performance.Our primary goal is to develop a network dependent message passing library called a Parallel Communication Library (PCL) that will exploit C-DAC's proprietary PARAMNet HCA features for efficient message transfer. Using PCL, we intend to observe the feasibility of the network and performance enhancement for additional features. The objective is to carry out different trials by implementing additional features and analyze the implications which would give us more insight towards suitability of transport offload/onload mechanism. This experimentation would give us feedbacks for designing the next generation architecture.
10.14529/jsfi160307;Parallel Processing Model for Cholesky Decomposition Algorithm in AlgoWiki Project;The comprehensive analysis of algorithmic properties of well-known. Cholesky decomposition was performed on the basis of multifold AlgoWiki technologies. There was performed a detailed analysis of information graph, data structure, memory access profile, computation locality, scalability and other algorithm properties, that allow us to demonstrate a lot of unevident properties split up. into machine-independent and machine-dependent subsets. A comprehension of the parallel algorithm structure provide us with the possibility to efficiently implement the algorithm at hardware platform specified.
10.14529/jsfi160308;Data merging for the cultural heritage imaging based on Chebfun approach;"Cultural heritage imaging has specific needs with regards to the analysis of images that require the manipulation of a single digital object that combines the images obtained from different instruments probing different scales at different wavelengths, with the further possibility of selecting two or three dimensional representations. We propose a unified imaging data processing approach based on the ""Chebyshev Technology"" using the open source software Chebfun which, by mapping data processing to simple polynomial transformations, brought considerable improvements over already existing procedures. Within that same data processing framework we may further investigate how to merge images originating from different acquisition devices since all images are expressed in the same basis (an approximate Chebfun polynomial basis ) before being merged. In the end, we hope to map all imaging data processing to simple polynomial operations. Our massive data-sets required parallelizing some Chebfun functions on GPUs, allowing about 100 times faster polynomial evaluation and up to 12 times faster on CPUs when parallelizing the whole algorithm."
10.14529/jsfi160401;In Situ Exploration of Particle Simulations with CPU Ray Tracing;We present a system for interactive in situ visualization of large particle simulations, suitable for general CPU-based HPC architectures. As simulations grow in scale, in situ methods are needed to alleviate IO bottlenecks and visualize data at full spatio-temporal resolution. We use a lightweight loosely-coupled layer serving distributed data from the simulation to a data-parallel renderer running in separate processes. Leveraging the OSPRay ray tracing framework for visualization and balanced P-k-d trees, we can render simulation data in real-time, as they arrive, with negligible memory overhead. This flexible solution allows users to perform exploratory in situ visualization on the same computational resources as the simulation code, on dedicated visualization clusters or remote workstations, via a standalone rendering client that can be connected or disconnected as needed.  We evaluate this system on simulations with up to 227M particles in the LAMMPS and Uintah computational frameworks, and show that our approach provides many of the advantages of tightly-coupled systems, with the flexibility to render on a wide variety of remote and coprocessing resources.
10.14529/jsfi160402;In Situ Visualization and Production of Extract Databases;Simulations running at high concurrency on HPC systems generate large volumes of data that are impractical to write to disk due to time and storage constraints. Applications often adapt by saving data infrequently, resulting in datasets with poor temporal resolution. This can make datasets difficult to interpret during post hoc visualization and analysis, or worse, it can lead to lost science. In Situ visualization and analysis can enable efficient production of small data products such as rendered images or surface extracts that consist of polygonal geometry plus fields. These data products are far smaller than their source data and can be processed much more economically in a traditional post hoc workflow using far fewer computational resources. We used the SENSEI and Libsim in situ infrastructures to implement rendering workflow and surface data extraction workflows in the AVF-LESLIE combustion code. These workflows were then demonstrated at high levels of concurrency and showed significant data reductions and limited impact on the simulation runtime.
10.14529/jsfi160403;In situ steerable hardwareindependent and datastructure agnostic visualization with ISAAC;The computation power of supercomputers grows faster than the bandwidth of their storage and network. Especially applications using hardware accelerators like Nvidia GPUs cannot save enough data to be analyzed in a later step. There is a high risk of loosing important scientific information. We introduce the in situ template library ISAAC which enables arbitrary applications like scientific simulations to live visualize their data without the need of deep copy operations or data transformation using the very same compute node and hardware accelerator the data is already residing on. Arbitrary meta data can be added to the renderings and user defined steering commands can be asynchronously sent back to the running application. Using a aggregating server, ISAAC streams the interactive visualization video and enables user to access their applications from everywhere.
10.14529/jsfi160404;Preparing for In Situ Processing on Upcoming Leadingedge Supercomputers;High performance computing applications are producing increasingly large amounts of data and placing enormous stress on current capabilities for traditional post-hoc visualization techniques. Because of the growing compute and I/O imbalance, data reductions, including in situ visualization, are required. These reduced data are used for analysis and visualization in a variety of different ways. Many of he visualization and analysis requirements are known a priori, but when they are not, scientists are dependent on the reduced data to accurately represent the simulation in post hoc analysis. The contributions of this paper is a description of the directions we are pursuing to assist a large scale fusion simulation code succeed on the next generation of supercomputers. These directions include the role of in situ processing for performing data reductions, as well as the tradeoffs between data size and data integrity within the context of complex operations in a typical scientific workflow.
10.14529/jsfi160405;Analysis of CPU Usage Data Properties and their possible impact on Performance Monitoring;CPU usage data (CPU user, system, iowait etc. load levels) are often the basic data used for performance monitoring. The source of these data is  the operating system. In this paper we analyze some properties of CPU usage data provided by Linux kernel. We examine kernel source code and provide test results to find which level of accuracy and precision one may expect when using CPU load level data.
10.14529/jsfi160406;Parallel algorithm for 3D modeling of monochromatic acoustic field by the method of integral equations;We present a parallel algorithm for solution of the three-dimensional Helmholtz equation in the frequency domain by the method of volume integral equations. The algorithm is applied to seismic forward modeling. The method of integral equations reduces the size of the problem by dividing the geologic model into the anomalous and background parts, but leads to a dense system matrix. A tolerable memory consumption and numerical complexity were achieved by applying an iterative solver, accompanied by an effective matrix-vector multiplication operation, based on the fast Fourier transform. We used OpenMP to speed up the matrix-vector multiplication, while MPI was used to speed up the equation system solver, and also for parallelizing across multiple sources. Practical examples and efficiency tests are presented.
10.14529/jsfi170101;Design and Implementation of the PULSAR Programming System for Large Scale Computing;The objective of the PULSAR project was to design a programming model suitable for large scale machines with complex memory hierarchies, and to deliver a prototype implementation of a runtime system supporting that model. PULSAR tackled the challenge by proposing a programming model based on systolic processing and virtualization. The PULSAR programming model is quite simple, with point-to-point channels as the main communication abstraction. The runtime implementation is very lightweight and fully distributed, and provides multithreading, message-passing and multi-GPU offload capabilities. Performance evaluation shows good scalability up to one thousand nodes with one thousand GPU accelerators.
10.14529/jsfi170102;Workflows for Science: a Challenge when Facing the Convergence of HPC and Big Data;Workflows have been used traditionally as a mean to describe and implement the computing usually parametric studies and explorations searching for the best solution  that  scientific researchers want to perform. A workflow is not only the computing application, but a way of documenting a process.  Science workflows may be of very different nature depending on the area of research, matching the actual experiment that the scientist want to perform. Workflow Management Systems are environments that offer the researchers tools to define, publish, execute and document their workflows. In some cases, the science workflows are used to generate data
10.14529/jsfi170103;A Survey: Runtime Software Systems for High Performance Computing;HPC system design and operation are challenged by the critical requirements for signicant advances in eciency, scalability, user productivity, and performance portability, even at the end of Moore's Law with approaching nano-scale semiconductor technology. Conventional practices employ distributed memory message passing programming interfaces, sometimes combining second level thread-based intra shared memory node interfaces such as OpenMP or with means of controlling heterogeneous components such as OpenCL for GPUs. While these methods include some modest runtime control, they are principally course grained and statically scheduled. Yet, performance for many real-world applications yield eciencies of less than 10% although some benchmarks may achieve 80% eciency or better (e.g., HPL). To address these challenges, strategies employing runtime software systems are being pursued to exploit information about the status of the application and the system hardware operation throughout the execution for purposes of introspection to guide the task scheduling and resource management in support of dynamic adaptive control. Runtime systems provide adaptive means to reduce the eects of starvation, latency, overhead, and contention. While each is unique in its details, many share common properties such as multi-tasking either preemptive or non-preemptive, message-driven computation such as active messages, sophisticated ne-grain synchronization such as dataow and futures contructs, global name or address spaces, and control policies for optimizing task scheduling in part to address the uncertainty of asynchrony. This survey will identify key parameters and properties of modern and sometimes experimental runtime systems actively employed today and provide a detailed description, summary, and comparison within a shared space of dimensions. It is not the intent of this paper to determine which is better or worse but rather to provide sucient detail to permit the reader to select among them according to individual need. 
10.14529/jsfi170104;xSDK Foundations: Toward an Extremescale Scientific Software Development Kit;Extreme-scale computational science increasingly demands multiscale and multiphysics formulations. Combining software developed by independent groups is imperative: no single team has resources for all predictive science and decision support capabilities. Scientific libraries provide high-quality, reusable software components for constructing applications with improved robustness and portability.  However, without coordination, many libraries cannot be easily composed.  Namespace collisions, inconsistent arguments, lack of third-party software versioning, and additional difficulties make composition costly.The Extreme-scale Scientific Software Development Kit (xSDK) defines community policies to improve code quality and compatibility across independently developed packages (hypre, PETSc, SuperLU, Trilinos, and Alquimia) and provides a foundation for addressing broader issues in software interoperability, performance portability, and sustainability.  The xSDK provides turnkey installation of member software and seamless combination of aggregate capabilities, and it marks first steps toward extreme-scale scientific software ecosystems from which future applications can be composed rapidly with assured quality and scalability.
10.14529/jsfi170105;Performance Portability of HPC Discovery Science Software: Fusion Energy Turbulence Simulations at Extreme Scale;As HPC R&D moves forward on a variety of path to exascale architectures today, an associated objective is to demonstrate performance portability of discovery-science-capable software.  Important application domains, such as Magnetic Fusion Energy (MFE), have improved modelling of increasingly complex physical systems -- especially with respect to reducing time-to-solution as well as  energy to solution.  The emergence of new insights on confinement scaling in MFE systems has been aided significantly by efficient software capable of harnessing powerful supercomputers to carry out simulations with unprecedented resolution and temporal duration to address increasing problem sizes.  Specifically, highly scalable particle-in-cell (PIC) programing methodology is used in this paper to demonstrate how modern scientific applications can achieve efficient architecture-dependent optimizations of performance scaling and code portability for path-to-exascale platforms.
10.14529/jsfi170201;Using High Performance Computing to Create and Freely Distribute the South Asian Genomic Database Necessary for Precision Medicine in this Population;"Precision medicine is an emerging approach for disease treatment and prevention that takes into account individual variability in genes, environment, and lifestyle for each person"". Efforts to implement precision medicine have gained traction in recent years due to significantly increased understanding of the role of genetic variations in human disease over the past decade. However, delivery of precision medicine requires robust population specific reference genome datasets for full appreciation of existing natural variation. The majority of publicly available genomic databases are primarily derived from Caucasian populations and do not fully address the diversity of Asian populations. In an effort to address this problem, we have aggregated and built a genomic database, ggcINDIA, specifically for South Asian populations. In collaboration with Global Alliance for Genomics and Health (GA4GH), we have made this database publicly available to the community through the GA4GH's Beacon project. ggcINDIA represents the first Beacon for South Asian populations. As more data is generated and aggregated, the ggcINDIA beacon will provide the precise genomic data that is critical to the delivery of precision medicine within South Asia."
10.14529/jsfi170202;An Application of GPU Acceleration in CFD Simulation for Insect Flight;The mobility and maneuverability of winged insects have been attracting attention, but the knowledge on the behavior of free-flying insects is still far from complete. This paper presents a computational study on the aerodynamics and kinematics of a free-flying model fruit-fly. An existing integrative computational fluid dynamics (CFD) framework was further developed using CUDA technology and adapted for the free flight simulation on heterogenous clusters.The application of general-purpose computing on graphics processing units (GPGPU) significantly accelerated the insect flight simulation and made it less computational expensive to find out the steady state of the flight using CFD approach.A variety of free flight scenarios has been simulated using the present numerical approach, including hovering, fast rectilinear flight, and complex maneuvers. The vortical flow surrounding the model fly in steady flight was visualized and analyzed. The present results showed good consistency with previous studies.
10.14529/jsfi170203;Simultac Fonton: A FineGrain Architecture for Extreme Performance beyond Moore's Law;"With nano-scale technology and Moore's Law end, architecture advance serves as the principal means of achieving enhanced efficiency and scalability into the exascale era. Ironically, the field that has demonstrated the greatest leaps of technology in the history of humankind, has retained its roots in its earliest strategy, the von Neumann architecture model which has imposed tradeoffs no longer valid for today's semiconductor technologies, although they were suitable through the 1980s. Essentially all commercial computers, including HPC, have been and are von Neumann derivatives. The bottlenecks imposed by this heritage are the emphasis on ALU/FPU utilization, single instruction issue and sequential consistency, and the separation of memory and processing logic (""von Neumann bottleneck""). Here the authors explore the possibility and implications of one class of non von Neumann architecture based on cellular structures, asynchronous multi-tasking, distributed shared memory, and message-driven computation. ""Continuum Computer Architecture"" is introduced as a genus of ultra-fine-grained architectures where complexity of operation is an emergent behavior of simplicity of design combined with highly replicated elements. An exemplar species of CCA, ""Simultac"" is considered comprising billions of simple elements, ""fontons"", of merged properties of data storage and movement combined with logical transformations. Employing the ParalleX execution model and a variation of the HPX+ runtime system software, the Simultac may provide the path to cost effective data analytics and machine learning as well as dynamic adaptive simulations in the trans-exaOPS performance regime."
10.14529/jsfi170204;The Simultaneous Transmit And Receive STAR Message Protocol;The STAR protocol is introduced, which solves three problems with MPI, a well known secur- ity problem, and three exascale communication problems. Optical implementations are developed compatible with 100 Gbit/sec Ethernet. Automatic fault resilience mechanisms are discussed, which improve HPC quality of service, and meet the exascale reliability and resilience challenges. The bandwidth problem for exascale computers interfacing with data centers is solved. The STAR protocol is combined with the SiMulPro core architecture (discussed in another article of this issue). The combination enables data centers, handheld computers, networked sensors, and super- computers, to be invulnerable to memory fault injection of viruses and rootkits.
10.14529/jsfi170205;Core Module Optimizing PDE Sparse Matrix Models With HPCG Example;This paper introduces a fundamentally new computer architecture for supercomputers. The core module is application compatible with an existing superscalar microprocessor, with minimized energy use, and is optimized for local sparse matrix operations. Optimized sparse matrix manip- ulation is discussed by analyzing the High Performance Conjugate Gradient (HPCG) benchmark speci...cation. This analysis shows how the DRAM memory wall is removed for this benchmark, and for sparse matrix models of partial di¤erential equations (PDEs) for a wide cross section of applications. By giving the programmer improved control over the con...guration of the super- computer, the potential for communication problems is minimized. Application compatibility is achieved while removing the superscalar instruction interpreter and multi-thread controller from the existing microprocessors hardware. These are transformed into compile-time utilities. The instruction cache is removed through an innovation in VLIW instruction processing. The data caches are unnecessary and are turned o¤ in order to optimally implement sparse matrix models.
10.14529/jsfi170206;Beating Floating Point at its Own Game: Posit Arithmetic;"A new data type called a posit is designed as a direct drop-in replacement for IEEE Standard 754 floating-point numbers (floats). Unlike earlier forms of universal number (unum) arithmetic, posits do not require interval arithmetic or variable size operands; like floats, they round if an answer is inexact. However, they provide compelling advantages over floats, including larger dynamic range, higher accuracy, better closure, bitwise identical results across systems, simpler hardware, and simpler exception handling. Posits never overflow to infinity or underflow to zero, and Not-a-Number (NaN) indicates an action instead of a bit pattern. A posit processing unit takes less circuitry than an IEEE float FPU. With lower power use and smaller silicon footprint, the posit operations per second (POPS) supported by a chip can be significantly higher than the FLOPS using similar hardware resources. GPU accelerators and Deep Learning processors, in particular, can do more per watt and per dollar with posits, yet deliver superior answer quality."
10.14529/jsfi170301;Resilience Design Patterns: A Structured Approach to Resilience at Extreme Scale;Reliability is a serious concern for future extreme-scale high-performance computing (HPC) systems. Projections based on the current generation of HPC systems and technology roadmaps suggest the prevalence of very high fault rates in future systems. The errors resulting from these faults will propagate and generate various kinds of failures, which may result in outcomes ranging from result corruptions to catastrophic application crashes. Therefore, the resilience challenge for extreme-scale HPC systems requires management of various hardware and software technologies that are capable of handling a broad set of fault models at accelerated fault rates. Also, due to practical limits on power consumption in HPC systems future systems are likely to embrace innovative architectures, increasing the levels of hardware and software complexities. As a result, the techniques that seek to improve resilience must navigate the complex trade-off space between resilience and the overheads to power consumption and performance. While the HPC community has developed various resilience solutions, application-level techniques as well as system-based solutions, the solution space of HPC resilience techniques remains fragmented. There are no formal methods and metrics to investigate and evaluate resilience holistically in HPC systems that consider impact scope, handling coverage, and performance & power efficiency across the system stack. Additionally, few of the current approaches are portable to newer architectures and software environments that will be deployed on future systems.
10.14529/jsfi170207;InfiniCortex  From Proofofconcept to Production;The global effort to build ever more powerful supercomputers is faced with the challenge of ramping up High Performance Computing systems to ExaScale capabilities and, at the same time, keeping the electrical power consumption for a system of that scale at less than 20 MW level. One possible solution, bypassing this local energy limit, is to use distributed supercomputers to alleviate intense power requirements at any single location. The other critical challenge faced by the global computer industry and international scientific collaborations is the requirement of streaming colossal amounts of time-critical data. Examples abound: i) transfer of astrophysical data collected by the Square Kilometre Array to the international partners, ii) streaming of large facilities experimental data through the Pacific Research Platform collaboration of DoE, ESnet and other partners in the US and elsewhere, iii) the Superficilities vision expressed by DoE, iv) new architecture for CERN LHC data processing pipeline focussing on more powerful processing facilities connected by higher throughput connectivity.
10.14529/jsfi170401;Towards A Data Centric System Architecture: SHARP;Increased system size and a greater reliance on utilizing system parallelism to achieve computational needs, requires innovative system architectures to meet the simulation challenges. The SHARP technology is a step towards a data-centric architecture, where data is manipulated throughout the system. This paper introduces a new SHARP optimization, and studies aspects that impact application performance in a data-centric environment. The use of UD-Multicast to distribute aggregation results is introduced, reducing the latency of an eight-byte MPI Allreduce() across 128 nodes by 16%. Use of reduction trees that avoid the inter-socket bus further improves the eight-byte MPI Allreduce() latency across 128 nodes, with 28 processes per node, by 18%. The distribution of latency across processes in the communicator is studied, as is the capacity of the system to process concurrent aggregation operations.
10.14529/jsfi170302;Performance Evaluation of Runtime Data Exploration Framework based on InSitu Particle Based Volume Rendering;We examine the performance of the in-situ data exploration framework based on the in-situ Particle Based Volume Rendering (In-Situ PBVR) on the latest many-core platform. In-Situ PBVR converts extreme scale volume data into small rendering primitive particle data via parallel Monte-Carlo sampling without costly visibility ordering. This feature avoids severe bottlenecks such as limited memory size per node and significant performance gap between computation and inter-node communication. In addition, remote in-situ data exploration is enabled by asynchronous file-based control sequences, which transfer the small particle data to client PCs, generate view-independent volume rendering images on client PCs, and change visualization parameters at runtime.
10.14529/jsfi170303;Development and Integration of an InSitu Framework for Flow Visualization of LargeScale Unsteady Phenomena in ICON;With large-scale simulation models on massively parallel supercomputers generating increasingly large data sets, in-situ visualization is a promising way to avoid bottlenecks. Enabling in-situ visualization in a simulation model asks for special attention to the interface between a parallel simulation model and the data analysis part of the visualization, and to presentation and interaction scenarios. Modifications to scientific workflows would potentially result in a paradigm shift, which affects compute and data intensive applications generally. We present our approach for enabling in-situ visualization within the highly parallelized climate model ICON using the DSVR visualization framework. We focus on the requirements for generalized grid and data structures, and for universal, scalable algorithms for volume and flow visualization of time series. In-situ pathline extraction as a technique for the visualization of unsteady flows has been integrated in the climate simulation model ICON and verified in first studies.
10.14529/jsfi170304;In Situ Visualization for 3D AgentBased Vocal Fold Inflammation and Repair Simulation;A fast and insightful visualization is essential in modeling biological system behaviors and understanding underlying inter-cellular mechanisms. High fidelity models produce billions of data points per time step, making in situ visualization techniques extremely desirable as they mitigate I/O bottlenecks and provide computational steering capability. In this work, we present a novel high-performance scheme to couple in situ visualization with the simulation of the vocal fold inflammation and repair using little to no extra cost in execution time or computing resources. The visualization component is first optimized with an adaptive sampling scheme to accelerate the rendering process while maintaining the precision of the displayed visual results. Our software employs VirtualGL to perform visualization in situ. The scheme overlaps visualization and simulation, resulting in the optimal utilization of computing resources. This results in an in situ system biology simulation suite capable of remote simulation of 17 million biological cells and 1.2 billion chemical data points, remote visualization of the results, and delivery of visualized frames with aggregated statistics to remote clients in real-time.
10.14529/jsfi170305;Seismic Processing Performance Analysis on Different Hardware Environment;In this research we have used computational-intensive software that implements 2D and 3D seismic migrations to study mini-application behavior for a set of the computational architectures. In addition to three architecture type comparative analysis, two CPU generation comparisons have been done.The dynamic behavior of chosen mini-applications was studied using BSC performance analysis tools to identify their common features. In summary, we observe the best performance of mini-applications on Intel Xeon E5-2698 CPU generation 4.  Intel Xeon Phi 7250 peculiar architectural characteristics requires careful source code optimizations to help the compiler to effectively vectorize time-consuming loops and to improve the cache locality in order to achieve higher performance level. Elbrus-4S CPU is theoretically suitable for such kind of applications, but currently observed performance is an order of magnitude less than on Xeon E5 family, we believe that the frequency and RAM bandwidth increasing, as well as source code optimization work could improve it's performance.
10.14529/jsfi170306;Exploring Scheduling Effects on Task Performance with TaskInsight;The complex memory hierarchies of nowadays machines make it very difficult to estimate the execution time of the tasks as depending on where the data is placed in memory, tasks of the same type may end up having different performance. Multiple scheduling heuristics have managed to improve performance by taking into account memory-related properties such as data locality and cache sharing. However, we may see tasks in certain applications or phases of applications that take little or no advantage of these optimizations. Without understanding when such optimizations are effective, we may trigger unnecessary overhead at runtime level.In previous work, we introduced TaskInsight, a technique to characterize how the memory behavior of the application is affected by different task schedulers through the analysis of data reuse across tasks. We now use this tool to dynamically trace the scheduling decisions of multithreaded applications through their execution and analyze how memory reuse can provide information on when and why locality-aware optimizations are effective and impact performance.We demonstrate how we can detect particular scheduling decisions that produced a variation in performance, and the underlying reasons when applying TaskInsight to several of the Montblanc benchmarks. This flexible insight is key both for the programmer and runtime to allow assigning the optimal scheduling policy to certain executions or phases.
10.14529/jsfi170307;From ProcessinginMemory to ProcessinginStorage;Near-data in-memory processing research has been gaining momentum in recent years. Typical processing-in-memory architecture places a single or several processing elements next to a volatile memory, enabling processing without transferring data to the host CPU. The increased bandwidth to and from volatile memory leads to performance gain. However processing-in-memory does not alleviate von Neumann bottleneck for big data problems, where datasets are too large to fit in main memory.   We present a novel processing-in-storage system based on Resistive Content Addressable Memory (ReCAM). It functions simultaneously as a mass storage and as a massively parallel associative processor. ReCAM processing-in-storage resolves the bandwidth wall by keeping computation inside the storage arrays, without transferring it up the memory hierarchy.   We show that ReCAM based processing-in-storage architecture may outperform existing processing-in-memory and accelerator based designs. ReCAM processing-in-storage implementation of Smith-Waterman DNA sequence alignment reaches a speedup of almost five over a GPU cluster. An implementation of in-storage inline data deduplication is presented and shown to achieve orders of magnitude higher throughput than traditional CPU and DRAM based systems.
10.14529/jsfi170402;Towards Decoupling the Selection of Compression Algorithms from Quality Constraints - An Investigation of Lossy Compression Efficiency;Data intense scientific domains use data compression to reduce the storage space needed. Lossless data compression preserves information accurately but lossy data compression can achieve much higher compression rates depending on the tolerable error margins. There are many ways of defining precision and to exploit this knowledge, therefore, the field of lossy compression is subject to active research. From the perspective of a scientist, the qualitative definition about the implied loss of data precision should only matter.With the Scientific Compression Library (SCIL), we are developing a meta-compressor that allows users to define various quantities for acceptable error and expected performance behavior. The library then picks a suitable chain of algorithms yielding the user's requirements, the ongoing work is a preliminary stage for the design of an adaptive selector. This approach is a crucial step towards a scientifically safe use of much-needed lossy data compression, because it disentangles the tasks of determining scientific characteristics of tolerable noise, from the task of determining an optimal compression strategy. Future algorithms can be used without changing application code.In this paper, we evaluate various lossy compression algorithms for compressing different scientific datasets (Isabel, ECHAM6), and focus on the analysis of synthetically created data that serves as blueprint for many observed datasets. We also briefly describe the available quantitiesof SCIL to define data precision and introduce two efficient compression algorithms for individualdata points. This shows that the best algorithm depends on user settings and data properties.
10.14529/jsfi170403;Adaptive Load Balancing Dashboard in Dynamic Distributed Systems;Considering the dynamic nature of new generation scientific problems, load balancing is a necessity to manage the load in an efficient manner. Load balancing systems are use to optimize the resource consumption, maximize the throughput, minimize response time, and to prevent overload in resources. In current research, we consider operational distributed systems with dynamic variables caused by different nature of the applications and heterogeneity of the various levels in the system. Conducted studies indicate that many different factors should be considered to select the load balancing algorithm, including the processing power, load transfer and communication delay of nodes. In this work, We aim to design a dashboard that is capable to merge the load balancing algorithms in different environments. We design an adaptive system infrastructure with the ability to adjust various factors in the run time of a load balancing algorithm. We propose a task and a resource allocation mechanism and further introduce a mathematical model of load balancing process in the system. We calculate a normalized hardware score that determines the maturity of system according to the environmental conditions of the load balancing process. Evaluation results confirm that the proposed method performs well and reduces the probability of system failure.
10.14529/jsfi170404;Additivity: A Selection Criterion for Performance Events for Reliable Energy Predictive Modeling;Performance events or performance monitoring counters (PMCs) are now the dominant predictor variables for modeling energy consumption. Modern hardware processors provide a large set of PMCs. Determination of the best subset of PMCs for energy predictive modeling is a non-trivial task given the fact that all the PMCs can not be determined using a single application run. Several techniques have been devised to address this challenge. While some techniques are based on a statistical methodology, some use expert advice to pick a subset (that may not necessarily be obtained in one application run) that, in experts' opinion, are significant contributors to energy consumption. However, the existing techniques have not considered a fundamental property of predictor variables that should have been applied in the first place to remove PMCs unfit for modeling energy. We address this oversight in this paper. We propose a novel selection criterion for PMCs called additivity, which can be used to determine the subset of PMCs that can potentially be used for reliable energy predictive modeling. It is based on the experimental observation that the energy consumption of a serial execution of two applications is the sum of energy consumptions observed for the individual execution of each application. A linear predictive energy model is consistent if and only if its predictor variables are additive in the sense that the vector of predictor variables for a serial execution of two applications is the sum of vectors for the individual execution of each application. The criterion, therefore, is based on a simple and intuitive rule that the value of a PMC for a serial execution of two applications is equal to the sum of its values obtained for the individual execution of each application. The PMC is branded as non-additive on a platform if there exists an application for which the calculated value differs significantly from the value observed for the application execution on the platform. The use of non-additive PMCs in a model renders it inconsistent. We study the additivity of PMCs offered by the popular state-of-the-art tools, Likwid and PAPI, by employing a detailed experimental methodology on a modern Intel Haswell multicore server CPU. We show that many PMCs in Likwid and PAPI that are widely used in models as key predictor variables are non-additive. This brings into question the reliability and the reported prediction accuracy of these models.
10.14529/jsfi170405;Cloud Service for Solution of Promising Problems of Nanotechnology;The paper presents the problem of creating a cloud service designed to solve promising nanotechnology problems on supercomputer systems. The motivation for creating such a service was the need to integrate ideas, knowledge and computing technologies related to this applied problem, as well as the need to involve specialists in solving problems of this class. The intermediate result of the work is a prototype of the cloud environment, implemented as a KIAM Multilogin service and an application software accessible from users virtual machines. The first applications of the service were the software packages GIMM_NANO and Flow_and_Particles, designed to solve the actual problems of nanoelectronics, laser nanotechnology, multiscale problems of applied gas dynamics. The implementation of the service took into account such aspects as support for parallel computations on the park of remote supercomputers, improving the efficiency of parallelization, very large data sets processing, visualization of supercomputer modeling results. With the help of the implemented service, it was possible to optimize the process of solving the applied problems associated with calculating the parameters of gas-dynamic flows in the microchannels of industrial spraying systems. In particular, it was possible to carry out the series of studies devoted to the analysis of gas-dynamic processes at the gas-metal boundary. In these studies it was shown that in the presence of microcapillaries in a technical system, it is necessary to use direct modeling of gas dynamic processes on the basis of the first principles in the Knudsen layers, for example, using molecular dynamics methods.
10.14529/jsfi200106;Building a Vision for Reproducibility in the Cyberinfrastructure Ecosystem: Leveraging Community Efforts;"The scientific computing community has long taken a leadership role in understanding and assessing the relationship of reproducibility to cyberinfrastructure, ensuring that computational results - such as those from simulations - are ""reproducible"", that is, the same results are obtained when one re-uses the same input data, methods, software and analysis conditions. Starting almost a decade ago, the community has regularly published and advocated for advances in this area. In this article we trace this thinking and relate it to current national efforts, including the 2019 National Academies of Science, Engineering, and Medicine report on ""Reproducibility and Replication in Science""."
10.14529/jsfi180102;RecordandReplay Techniques for HPC Systems: A Survey;Record-and-replay techniques provide the ability to record executions of nondeterministic applications and re-execute them identically. These techniques find use in the contexts of debugging, reproducibility, and fault-tolerance, especially in the presence of nondeterministic factors such as message races. Record-and-replay techniques are highly diverse in terms of the fidelity of replay they provide, the assumptions they make about the recorded application, the programming models they target, and the runtime overheads they impose.                                                                           
10.14529/jsfi180103;Survey of Storage Systems for HighPerformance Computing;"In current supercomputers, storage is typically provided by parallel distributed file systems for hot data and tape archives for cold data. These file systems are often compatible with local file systems due to their use of the POSIX interface and semantics, which eases development and debugging because applications can easily run both on workstations and supercomputers. There is a wide variety of file systems to choose from, each tuned for different use cases and implementing different optimizations. However, the overall application performance is often held back by I/O bottlenecks due to insufficient performance of file systems or I/O libraries for highly parallel workloads. Performance problems are dealt with using novel storage hardware technologies as well as alternative I/O semantics and interfaces. These approaches have to be integrated into the storage stack seamlessly to make them convenient to use. Upcoming storage systems abandon the traditional POSIX interface and semantics in favor of alternative concepts such as object and key-value storage; moreover, they heavily rely on technologies such as NVM and burst buffers to improve performance. Additional tiers of storage hardware will increase the importance of hierarchical storage management. Many of these changes will be disruptive and require application developers to rethink their approaches to data management and I/O. A thorough understanding of today's storage infrastructures, including their strengths and weaknesses, is crucially important for designing and implementing scalable storage systems suitable for demands of exascale computing."
10.14529/jsfi180104;The HighQ Club: Experience with Extremescaling Application Codes;J?lich Supercomputing Centre (JSC) started running (extreme) scaling workshops with its first IBM Blue Gene supercomputer, finally spanning three generations each seeing an increase in the number of cores and available threads. Over the years, this workshop series attracted numerous international code teams and resulted in many applications capable of running on all available cores of each system.
10.14529/jsfi180105;Exploiting the Performance Benefits of Storage Class Memory for HPC and HPDA Workflows;"Byte-addressable storage class memory (SCM) is an upcoming technology that will transform the memory and storage hierarchy of HPC systems by dramatically reducing the latency gap between DRAM and persistent storage. In this paper, we discuss general SCM characteristics, including the different hardware configurations and data access mechanisms SCM is likely to provide. We outline the performance challenges I/O requirements place on traditional scientific workflows and present how data access through SCM can have a beneficial impact on the performance of such workflows, in particular those with large scale data dependencies. We describe the system software components that are required to enabled workflow and data aware resource allocation scheduling in order to optimise both system throughput and time to solution for individual applications; these include a data scheduler and data movers. We also present an illustration of the performance improvement potential of the technology, based on initial workflow performance benchmarks with I/O dependencies."
10.14529/jsfi180106;A General Guide to Applying Machine Learning to Computer Architecture;The resurgence of machine learning since the late 1990s has been enabled by significant advances in computing performance and the growth of big data. The ability of these algorithms to detect complex patterns in data which are extremely difficult to achieve manually, helps to produce effective predictive models. Whilst computer architects have been accelerating the performance of machine learning algorithms with GPUs and custom hardware, there have been few implementations leveraging these algorithms to improve the computer system performance. The work that has been conducted, however, has produced considerably promising results. The purpose of this paper is to serve as a foundational base and guide to future computer architecture research seeking to make use of machine learning models for improving system efficiency. We describe a method that highlights when, why, and how to utilize machine learning models for improving system performance and provide a relevant example showcasing the effectiveness of applying machine learning in computer architecture. We describe a process of data generation every execution quantum and parameter engineering. This is followed by a survey of a set of popular machine learning models. We discuss their strengths and weaknesses and provide an evaluation of implementations for the purpose of creating a workload performance predictor for different core types in an x86 processor. The predictions can then be exploited by a scheduler for heterogeneous processors to improve the system throughput. The algorithms of focus are stochastic gradient descent based linear regression, decision trees, random forests, artificial neural networks, and k-nearest neighbors.
10.14529/jsfi190105;Facilitating HPC Operation and Administration via Cloud;Experiencing a tremendous growth, Cloud Computing offers a number of advantages over other distributed platforms. Introducing the advantages of High Performance Computing (HPC) also brought forward the development of HPCaaS (HPC as a Service), which has mainly focused on flexible access to resources, cost-effectiveness, and the no-maintenance-needed for end-users. Besides providing and using HPCaaS, HPC centers could leverage more from Cloud Computing technology, for instance to facilitate operation and administration of deployed HPC systems, commonly faced by most supercomputer centers.
10.14529/jsfi180101;AlgoWiki Project as an Extension of the Top500 Methodology;The AlgoWiki project is dedicated to describing the parallel structure and key features of various algorithms. The descriptions are intended to provide complete information about algorithm's properties, which are needed to adequately assess their implementation efficiency for any computing platform. This work sets out the key areas for further development of the project which were recently developed based on working with the AlgoWiki encyclopedia. We are suggesting an approach to extend the Top500 methodology, which is commonly used to compare various computing platforms.
10.14529/jsfi180204;Parallel Numerical Algorithm for Solving Advection Equation for Coagulating Particles;In this work we present a parallel implementation of numerical algorithm solving the Cauchy problem for equation of advection of coagulating particles. This equation describes time-evolution of the concentration f(x, v, t) of particles of size v at the point x at the time-moment t. Our numerical algorithm is based on use of total variation diminishing (TVD) scheme and perfectly matching layers (PML) for approximation of advection operator along spatial coordinate x and utilization of the fast numerical method for evaluation of coagulation integrals exploiting low-rank decomposition of coagulation kernel coefficients and fast FFT-based implementation of convolution operation along particle size coordinate v. In our work we exploit one-dimensional domain decomposition approach along spatial coordinate x because it allows to avoid use of parallel FFT implementations which are very expensive in terms of data exchanges and have poor parallel scalability. Moreover, locality of finite-difference operator from TVD-scheme along x coordinate allows to obtain good scalability even for computing clusters with slow network interconnect due to modest volumes of data necessary for synchronization exchanges between times integration steps.
10.14529/jsfi180206;High Performance Computing with Coarse Grained Model of Biological Macromolecules;The Unified Coarse Grained Model of biological macromolecules (UCGM) that is being developed in our laboratory is a model designed to carry out large-scale simulations of biological macromolecules. The simplified chain representation used in the model allows to obtain 3-4 orders of magnitude extention of the time-scale of simulations, compared to that of all-atom simulations. Unlike most of the other coarse-grained force fields, UCGM is a physics-based force field, independent of structural databases and applicable to treat non-standard systems. In this communication, the efficiency and scalability of the new version of UCGM package with Fortran 90, with two parallelization levels: coarse-grained and fine-grained, is reported for systems with various size and oligomeric state. The performance was tested in the canonical- and replica exchange MD mode, with small- and moderate-size proteins and protein complexes (20 to 1,636 amino-acid residues), as well as with large systems such as, e.g., human proteosome 20S with size over 6,200 aminoacid residues, which show the advantage of using coarse-graining. It is demonstrated that, with using massively parallel architectures, and owing to the physics-based nature of UCGM, real-time simulations of the behavior of subcellular systems are feasible.
10.14529/jsfi180208;Numerical Simulations of Black Hole Accretion Flows;We model the structure and evolution of black hole accretion disks using numerical simulations. The numerics is governed by the equations of general relativistic magneto-hydrodynamics (GRMHD). Accretion disks and outflows can be found at the base of very energetic ultra-relativistic jets produced by cosmic explosions, so called gamma-ray bursts (GRBs). Another type of phenomena are blazars, with jets emitted from the centers of galaxies.Long-lasting, detailed computations are essential to determine the physics of these explosions, and confront the theory with potential observables. From the point of view of numerical methods and techniques, three ingredients need to be considered. First, the numerical scheme must work in a conservative manner, which is achieved by solving a set of non-linear equations to advance the conserved quantities from one time step to the next. Second, the efficiency of computations depends on the code parallelization methods. Third, the analysis of results is possible via the post-processing of computed physical quantities, and visualization of the flow properties. This is done via implementing packages and libraries that are standardized in the field of computational astrophysics and supported by community developers.In this paper, we discuss the physics of the cosmic sources. We also describe our numerical framework and some technical issues, in the context of the GRMHD code which we develop. We also present a suite of performance tests, done on the High-Performance Computer cluster (HPC) in the Center for Mathematical Modeling of the Warsaw University.
10.14529/jsfi180203;On the Inversion of Multiple Matrices on GPU in Batched Mode;In this research we are considering the benchmarking of batched matrix inversion and solution of linear systems. The problem of multiple matrix inversion with the same fill sparsity is usually considered in problems of fluid mechanics with chemistry. In this case the system is stiff, and an implicit method is required to solve the problem. The core of such method is the multiple matrix inversion. We benchmark different methods based on cuSPARSE and MAGMA libraries and CPU LAPACK version depending on the matrix filling. We also provide our own experimental code that implements GaussJordan elimination on GPU using register shuffle. It is shown that the fastest method is the QR matrix inversion for single precision calculations. We also show that the suggested Gauss-Jordan elimination method looks promising being about 8-10 times faster than cuSPARSE QR method. We also demonstrate the application of batch solvers in the coupled reactive flow problem.
10.14529/jsfi180401;Adaptive Load Balancing in the Modified Mind Evolutionary Computation Algorithm;The paper presents an adaptive load balancing method for the modified parallel Mind Evolutionary Computation (MEC) algorithm. The proposed method takes into account an objective function's topology utilizing the information obtained during the landscape analysis stage as well as the information on available computational resources. The modified MEC algorithm and proposed static load balancing method are designed for loosely coupled parallel computing systems and imply a minimal number of interactions between computational nodes when solving global optimization problems. A description of the proposed method is presented in this work along with the results of computational experiments, which were carried out with a use of multi-dimensional benchmark functions of various classes. Obtained results demonstrate that an effective use of available computational resources in the proposed method helps finding a better solution comparing to the traditional parallel MEC algorithm balancing. Further development of the proposed method requires more advanced termination criteria in order to avoid excessive iterations.
10.14529/jsfi180202;Scalability Evaluation of Cimmino Algorithm for Solving Linear Inequality Systems on Multiprocessors with Distributed Memory;The paper is devoted to a scalability study of Cimmino algorithm for linear inequality systems. This algorithm belongs to the class of iterative projection algorithms. For the analytical analysis of the scalability, the BSF (Bulk Synchronous Farm) parallel computation model is used. An implementation of the Cimmino algorithm in the form of operations on lists using higher-order functions Map and Reduce is presented. An analytical estimation of the scalability boundary of the algorithm for cluster computing systems is derived. An information about the implementation of Cimmino algorithm on lists in C++ language using the BSF program skeleton and MPI parallel programming library is given. The results of large-scale computational experiments performed on a cluster computing system are demonstrated. A conclusion about the adequacy of the analytical estimations by comparing them with the results of computational experiments is made.
10.14529/jsfi180207;3D Problems of Rotating Detonation Wave in a Ramjet Engine Modeled on a Supercomputer;A rotating detonation engine (RDE) combustion chamber was modeled in the work numerically using 3D geometry. The RDE is a new type of engines capable to create higher thrust than the traditional ones, which are based on the combustible mixture deflagration process. In the numerical experiment, different scenarios of the engine performance were obtained. The calculations were made at a compact super-computer APK-5 with a peak performance of 5.5 Tera Flops.
10.14529/jsfi180201;Deep Analysis of Job State Statistics on Lomonosov2 Supercomputer;It is a common knowledge that the increasingly growing capabilities of HPC systems are always limited by a number of efficiency related issues. The reasons can be very different: hardware failures, incorrect job scheduling, peculiarities of algorithm, chosen programming technology specifics, etc. Most of these issues can be detected after precise analysis, but is a very resourceful way to study every application run. Therefore we performed less complicated analysis of the whole supercomputer job flow. In this paper we share our experience of analyzing user applications' job states assigned by the SLURM resource manager that is used on the Lomonosov-2 system at Supercomputing center of Lomonosov Moscow State University. The statistics on job states was collected and it revealed that the ratio of correctly finished jobs (with the COMPLETED state) was rather low. The jobs owners were asked if the distribution of their jobs' states is normal regarding their applications. This user feedback was processed, and some new ways of efficiency gain were revealed as the result.
10.14529/jsfi180205;Generation of Multiple Turbulent Flow States for the Simulations with Ensemble Averaging;The paper deals with the problem of improving the performance of high-fidelity incompressible turbulent flow simulations on high performance computing systems. The ensemble averaging approach, combining averaging in time together with averaging over multiple ensembles, allows to speedup the corresponding simulations by increasing the computing intensity of the numerical method (flops per byte ratio). The current paper focuses on further improvement of the proposed computational methodology, and particularly, on the optimization of procedure to generate multiple independent turbulent flow states.
10.14529/jsfi180402;Multicore Platform Efficiency Across Remote Sensing Applications;A wide range of modern system architectures and platforms targeted for different algorithms and application areas is now available.Even general-purpose systems have advantages in some computation areas and bottlenecks in another. Scientific applications on specific areas, on the other hand, have different requirements for CPU performance, scalability and power consumption.The best practice now is algorithm/architecture co-exploration approach, where scientific problem requirements influence the hardware configuration
10.14529/jsfi180403;A Study on CrossArchitectural Modelling of Power Consumption Using Neural Networks;On the path to Exascale, the goal of High Performance Computing (HPC) to achieve maximum performance becomes the goal of achieving maximum performance under strict power constraint. Novel approaches to hardware and software co-design of modern HPC systems have to be developed to address such challenges. In this paper, we study prediction of power consumption of HPC systems using metrics obtained from hardware performance counters. We argue that this methodology is portable across different micro architecture implementations and compare results obtained on Intel 64, IBMR and Cavium ThunderXR ARMv8 microarchitectures.We discuss optimal number and type of hardware performance counters required to accurately predict power consumption.We compare accuracy of power predictions provided by models based on Linear Regression (LR) and Neural Networks (NN). We find that the NN-based model provides better accuracy of predictions than the LR model. We also find, that presently it is not yet possible to predict power consumption on a given microarchitecture using data obtained on a different microarchitecture. Results of our work can be used as a starting point for developing unified, cross-architectural models for predicting power consumption.
10.14529/jsfi180301;Continuum Computing  on a New Performance Trajectory beyond Exascale;The end of Moore's Law is a cliche that none the less is a hard barrier to future scaling of high performance computing systems. A factor of about 4x in device density is all that is left of this form of improved throughput with a 5x gain required just to get to the milestone of exascale. The remaining sources of performance improvement are better delivered efficiency of more than 10x and alternative architectures to make better use of chip real estate. This paper will discuss the set of principles guiding a potential future of non-von Neumann architectures as adopted by the experimental class of Continuum Computer Architecture (CCA). It is being explored by the Semantic Memory Architecture Research Team (SMART) at Indiana University. CCA comprises a homogeneous aggregation of cellular components (function cells) which are orders of magnitude smaller than lightweight cores and individually is unable to accomplish a computation but in combination can do so with extreme cost efficiency and unprecedented scalability. It will be seen that a path exists based on such unconventional methods like neuromorphic computing or dataflow that not only will meet the likely exascale milestone in the same time with much better power, cost, and size but also will set a new performance trajectory leading to Zetaflops capability before 2030.
10.14529/jsfi180404;Autotuning Techniques for PerformancePortable Point Set Registration in 3D;We present an autotuning approach applied to exhaustive performance engineering of the EM-ICP algorithm for the point set registration problem with a known reference. We were able to achieve progressively higher performance levels through a variety of code transformations and an automated procedure of generating a large number of implementation variants. Furthermore, we managed to exploit code patterns that are not common when only attempting manual optimization but which yielded in our tests better performance for the chosen registration algorithm. Finally, we also show how we maintained high levels of the performance rate in a portable fashion across a wide range of hardware platforms including multicore, manycore coprocessors, and accelerators. Each of these hardware classes is much different from the others and, consequently, cannot reliably be mastered by a single developer in a short time required to deliver a close-to-optimal implementation. We assert in our concluding remarks that our methodology as well as the presented tools provide a valid automation system for software optimization tasks on modern HPC hardware.
10.14529/jsfi180302;Threedimensional Inversion of Electromagnetic Geophysical Data with Parallel Computational Code on Supercomputer Complex Lomonosov;"Usage of 2D inversion of magnetotelluric data for real geological objects can cause distortion, but it is more often used in commercial projects, because of its effectiveness and great experience. Whereas in the case of 3D inversion is not such a great experience and there are a number of global problems. When switching to 3D inversion of MT data, the requirement for computer technology is significantly increased. In this paper we will discuss a few examples of 3D inversion of electromagnetic geophysical field data with the usage of Lomonosov supercomputer and show its effectiveness on several geological objects. Each object is associated with a variety of problems: from search for shallow ore to regional hydrocarbon exploration. But all these objects contain a large volume of measurements obtaining qualitative results for which requires a huge amount of time. So that the use of 3D inversion with a high-performance computational complex makes it possible to obtain a qualitative result of solving a wide range of problems."""""
10.14529/jsfi180303;Simulating the Longtimescale Structural Behavior of Bacterial and Influenza Neuraminidases with Different HPC Resources;"Understanding the conformational dynamics which affects ligand binding by Neuraminidases is needed to improve the in silico selection of novel drug candidates targeting these pathogenicity factors and to adequately estimate the efficacy of potential drugs. Conventional molecular dynamics (MD) is a powerful tool to study conformational sampling, drug-target recognition and binding, but requires significant computational effort to reach timescales relevant for biology. In this work the advances in a computer power and specialized architectures were evaluated at simulating long MD trajectories of the structural behavior of Neuraminidases. We conclude that modern GPU accelerators enable calculations at the timescales that would previously have been intractable, providing routine access to microsecond-long trajectories in a daily laboratory practice. This opens an opportunity to move away from the ""static"" affinity-driven strategies in drug design towards a deeper understanding of ligand-specific conformational adaptation of target sites in protein structures, leading to a better selection of efficient drug candidates in silico. However, the performance of modern GPUs is yet far behind the deeply-specialized supercomputers co-designed for MD. Further development of affordable specialized architectures is needed to move towards the much-desired millisecond timescale to simulate large proteins at a daily routine."
10.14529/jsfi180304;Parallel GPUbased Implementation of OneWay Wave Equation Migration;"We present an original algorithm for seismic imaging, based on the depth wavefield extrapolation by the  one-way wave equation. Parallel implementation of the algorithm is based on the several levels of parallelism. The input data parallelism allows processing full coverage for some area (up to one square km); thus, data are divided into several subsets and each subset is processed by a single MPI process. The mathematical approach allows dealing with each frequency independently and treating solution layer-by-layer; thus, a set of 2D cross-sections instead of the initial 3D common-offset vector gathers are processed simultaneously. This part of the algorithm is implemented suing GPU. Next, each common-offset vector image can be stacked, processed and stored independently. As a result, we designed and implemented the parallel algorithm based on the use of CPU-GPU architecture which allows computing common-offset vector images using one-way wave equation-based amplitude preserving migration. The algorithm was used to compute seismic images from real seismic land data."
10.14529/jsfi180305;Highperformance Computational Modeling of Chromosome Structure;We present a polymer modeling approach to generate the ensemble of 3D chromosome conformations at different time points of mitosis-interphase transition. Dynamics of structure during mitosis-G1 transition indicates quick and slow stages of chromosome shape alterations. At intermediate and late time scale the changes in chromosome compaction are small. To assess time dependence of contact map establishment during G1 we calculate contact maps at different times after mitotic decondensation. We demonstrate that the patterns of contacts observed soon after mitotic decondensation remain similar during G1. Whole contact map for mouse chromosome 18 at late G1 time correlates with the experimental chromosome conformation capture data. The simulations reproduce the main experimental findings, contact map persistence during G1 as well as specific pattern of long-range interactions in interphase chromosome. Our results suggest that spatial compartmentalization of an interphase chromosome is driven by interactions between different types of megabase sized chromatin domains during the formation of globular chromosome state at the end of mitotis to G1 transition.
10.14529/jsfi180306;Numerical Simulations of Structural Chromosomal Instability;"The origin of dose-response curves for radiation-induced chromosomal instability (CI) is studied using the mechanistic CI model. The model takes into account DNA damage generation and repair in the progeny of irradiated cells and cell passage through mitotic cycle. We consider the formation of DNA double-strand breaks (DSBs) de novo in the S phase, where predominantly chromatid-type aberrations are formed. Among them sister chromatid exchanges of the ""isochromatid deletion"" type, or ""chromatid dicentrics"" are of primary interest. When the cell enters mitosis, the fate of chromosomal aberrations depends on their types. Chromosomal and chromatid fragments, having entered mitosis, either are transmitted into one of the daughter cells, or are lost. A chromatid dicentric in mitosis forms an anaphase bridge. These mechanistic assumptions were used to demonstrate that the dose-response curves are closely related to the dynamic curves for CI. The principles underlying this relationship are analyzed."
10.14529/jsfi180307;GPUbased Implementation of Discrete Element Method for Simulation of the Geological Fault Geometry and Position;We present an algorithm for numerical simulation of the geological fault formation. The approach is based on the discrete elements method, which allows modeling of the deformations and structural discontinuity of the Upper part of the Earth crust. In the discrete elements method, the medium is represented as an combination of discrete particles which interact as elastic or viscoelastic bodies. Additionally, external potential forces, for example gravitational forces, may be introduced. At each time step the full set of forces acting at each particle is computed, after that the position of the particle is evaluated on the base of Newtonian mechanics. We implement the algorithm using CUDA technology to simulate single statistical realization of the model, whereas MPI is used to parallelize with respect to different statistical realizations. Obtained numerical results show that for low dip angles of the tectonic displacements relatively narrow faults form, whereas high dip angles of the tectonic displacements lead to a wide V-shaped deformation zones.
10.14529/jsfi180308;Modelling of Quantum Qubit Behaviour for Future Quantum Computers;This work deals with quantum qubit modelling based on a silicon material with embedded phosphorus atoms because a future quantum computer can be built on the basis of this qubit. The building of atomic models of bulk crystalline silicon and silicene, as well as calculation of their total energies, were performed using the Quantum ESPRESSO software package, using highperformance computing (HPC). For silicon and phosphorus atoms the generalized gradient approximation (GGA) was used in terms of the spin-orbit non-collinear interaction by means of the Quantum ESPRESSO package. The equilibrium orientations of the phosphorus qubit spins and localization of the wave functions in the 2D and bulk crystalline silicon phases were theoretically investigated by means of quantum-mechanical calculations. The existence of an exchange interaction between qubits has been confirmed, which leads to a change in the wave function's localization and spin orientation, and in the case of silicene, this interaction was stronger.
10.14529/jsfi180405;Benchmarking Quantum Chemistry Methods in Calculations of Electronic Excitations;Quantum chemistry methods are applied to obtain numerical solutions of the Schrodinger equation for molecular systems. Calculations of transitions between electronic states of large molecules present one of the greatest challenges in this field which require the use of supercomputer resources. In this work we describe the results of benchmark calculations of electronic excitation in the protein domains which were designed to engineer novel fluorescent markers operating in the near-infrared region. We demonstrate that such complex systems can be efficiently modeled with the hybrid qunatum mechanics/molecular mechanics approach (QM/MM) using the modern supercomputers. More specifically, the time-dependent density functional theory (TD-DFT) method was primarily tested with respect to its performance and accuracy. GAMESS (US) and NWChem software were benchmarked in direct and storage-based TDDFT calculations with the hybrid B3LYP density functional, both showing good scaling up to 32 nodes. We note that conventional SCF calculations greatly outperform direct SCF calculations for our test system. Accuracy of TD-DFT excitation energies was estimated by a comparison to the more accurate ab initio XMCQDPT2 method.
10.14529/jsfi180309;Multiscale Simulations Approach: Crosslinked Polymer Matrices;Atomistic molecular dynamics simulations can usually cover only a very limited range in space and time. Thus, the materials like polymer resin networks, the properties of which are formed on macroscopic scale, are hard to study thoroughly using only molecular dynamics. Our work presents a multiscale simulation methodology to overcome this shortcoming. To demonstrate its effectiveness, we conducted a study of thermal and mechanical properties of complex polymer matrices and establish a direct correspondence between simulations and experimental results. We believe this methodology can be successfully used for predictive simulations of a broad range of polymer matrices in glassy state.
10.14529/jsfi180310;Application of High Performance Computing for Comparison of Two Highly Branched Lysine Molecules of Different Topology;High performance computations were performed for comparison of size and other properties of big heavily charged biocompatible molecules of complex topology in water. Lysine dendrimer and short dendritic brush of the same molecular weight were studied by molecular dynamics simulation method and GROMACS software package. The size and structural properties of these two systems were compared. It was shown that dendritic brush has smaller size and more dense core than the dendrimer. Radial density profile for both molecules is not monotonous and has minimum near core of molecules. This minimum is wider and deeper for dendrimer than for dendritic brush. Thus dendrimer has larger region of low density than dendritic brush and is more suitable for use for encapsulation and delivery of hydrophobic drugs.
10.14529/jsfi180311;Developing Efficient Implementations of Bellman-Ford and ForwardBackward Graph Algorithms for NEC SXACE;The main goal of this work is to demonstrate that the development of data-intensive appli- cations for vector systems is not only important and interesting, but is also very possible. In this paper we describe possible implementations of two fundamental graph-processing algorithms for an NEC SX-ACE vector computer: the BellmanFord algorithm for single source shortest paths computation and the Forward-Backward algorithm for strongly connected components detection. The proposed implementations have been developed and optimised in accordance with features and properties of the target architecture, which allowed them to achieve performance comparable to other traditional platforms, such as Intel Skylake, Intel Knight Landing or IBM Power processors.
10.14529/jsfi180312;Applications of High Performance Computing: Born-Oppenheimer Molecular Dynamics of Complex Formation in Aqueous Solutions;The progress of supercomputer technologies initiated the development of methods of computational chemistry and their applications, particularly molecular dynamic simulations with ab initio potentials. These new methods allow to solve important problems of chemistry and technology. Particularly, solvent extraction and separation techniques are widely used to decrease the amount of radioactive wastes, especially radioactive caesium isotopes present in liquid phases. We demonstrated that the calculated binding constants between the alkali cation and calix[4]arene differ 103 times for Cs+ and Na+ ions, that is in good agreement with the experimental value. We report the results of benchmark calculations of our model system composed of 929 atoms described in the  density functional theory approximation with the GGA-type functional PBE with the empirical dispersion correction D3 and combined basis of Gaussian functions and plane waves DZVP with Goedecker-Teter-Hutter pseudopotentials. We demonstrate that efficiency of calculations decrease to about half if the amount of nodes is 16 on the Lomonosov-2 supercomputer.
10.14529/jsfi180313;Analysis of the Effect of Dispersion Forces on the Dielectric Film Properties Using Parallel Computing;The paper presents the analysis of dispersion forces effect on local properties in thin free films. Using a Coupled Fluctuated Dipole Method with developed methods for numerical calculations of dielectric properties, the films with different lateral sizes and thicknesses were studied. In particular, the molecular polarizabilities at different distance from the film interface were analyzed. It was shown that dispersion interaction between the molecules, even for the case of nonpolar liquid with weak intermolecular interactions, causes a notable variation in dielectric properties of thin film, which is associated with the boundary layer formation. This variation, in turn, causes a strong dependence of polarizability accuracy on the cut-off radius. It is demonstrated that parallel computing algorithms can be effectively applied for obtaining the reliable data on properties of liquids in wetting films and boundary layers even under resource-imposed constraint on the size of ensemble of molecules to be handled in the numerical studies.
10.14529/jsfi180314;Quantum Chemistry Research of Interaction between 3DTransition Metal Ions and a Defective Graphene on the Supercomputer Base;Quantum chemistry research is presented in the article, and it concerns the interaction within the complexes formed by the defective graphene clusters and ions of 3d-transition metals V,Cr,Mn, Fe,Co,Ni,Cu. The charges of all regarded ions were +1. All calculations were made at UDFT B3LYP/6-31G level of theory with the BSSE error taken into account. The strongest interaction with the defective clusters is observed in the case of Co+ ion. At the same time, this ion has demonstrated rather weak interaction with the defect-free graphene. Thus, the presence of Co+ in the reaction media increases probability of defect formation with the further forming of short nanotubes and curved carbon clusters with complex topology of their own.
10.14529/jsfi180315;A Flux Splitting Method for the SHTC Model for Highperformance Simulations of Twophase Flows;In this paper we propose a new flux splitting approach for the symmetric hyperbolic thermodynamically compatible (SHTC) equations of compressible two-phase flow which can be used in finite-volume methods. The approach is based on splitting the entire model into acoustic and pseudo-convective submodels. The associated acoustic system is numerically solved applying HLLC-type Riemann solver for its Lagrangian form. The convective part of the pseudo-convective submodel is solved by a standart upwind scheme. For other parts of the pseudo-convective submodel we apply the FORCE method. A comparison is carried out with unsplit methods. Numerical results are obtained on several test problems. Results show good agreement with exact solutions and reference calculations.
10.14529/jsfi180316;Magnetic Properties of LaAlO3/SrTiO3 Heterostructure Modelled on a Supercomputer;The oxide heterostructure composed of LaAlO3 (LAO) thin film on top of SrTiO3 (STO) substrate is the best known example of a system where a metallic state is formed in the STO layers next to the interface [1]. In the frame of present work we analyze an impact of oxygen vacancies and hydrogen dopants located in the AlO2 surface layer and in the TiO2 interfacial plane of LAO/STO heterostructure onto the magnetic properties by performing spin-polarized calculations based on density functional theory (DFT). We found stable local magnetic moments formed within atomically thin magnetic layers at the interface. We confirmed that agnetism can be generated by oxygen vacancies located either at the surface or at the interface. In addition, we demonstrate magnetic moments formation by hydrogen dopants located at the interface. Finally, the case of two defects combination was investigated, when negligibly small magnetic moment induction was found to take place.
10.14529/jsfi180317;Optimization of BWB Aircraft Using Parallel Computing;Nacelle shape optimization for Blended Wing Body (BWB) is performed. The optimization procedure is based on numerical calculations of the Reynolds-averaged Navier-Stokes equations. For the Top Level Aircraft Requirements, formulated in AGILE project, the propulsion system was designed. The optimization procedure was divided in two steps. At first step, the isolated nacelle was designed and optimized for cruise regimes. This step is listed in paragraph 3. At second step the nacelles positions over airframe were optimized. To find the optimum solution, surrogate-based Efficient Global Optimization algorithm is used. An automatic structural computational mesh creation is realized for the effective optimization algorithm working. This whole procedure is considered in the context of the third generation multidisciplinary optimization techniques, developed within AGILE project. During the project, new techniques should be implemented for the novel aircraft configurations, chosen as test cases for application of AGILE technologies. It is shown that the optimization technology meets all requirements and is suitable for using in the AGILE project.
10.14529/jsfi180318;Supercomputer Simulations of Nondestructive Tomographic Imaging with Rotating Transducers;"A method of nondestructive ultrasound tomographic imaging employing a rotating transducer system is proposed. The rotating transducer system increases the number of emitters and detectors in a tomographic scheme by several times and makes it possible to neutralize image artifacts resulting from incomplete-data tomography. The inverse problem of tomographic reconstructing the velocity structure inside the inspected object is considered as a nonlinear coefficient inverse problem for a scalar wave equation. Scalable iterative algorithms for reconstructing the longitudinal wave velocity inside the object are discussed. The methods are based on the explicit representation for the gradient of the residual functional. The algorithms employ parallelizing the computations over emitter positions. Numerical simulations performed on the ""Lomonosov-2"" supercomputer showed that the tomographic methods developed can not only detect boundaries of defects, but also determine the wave velocity distribution inside the defects with high accuracy provided that both reflected and transmitted waves are registered."
10.14529/jsfi180319;Reverse Mapping Algorithm for Multiscale Numerical Simulation of Polylactic Acid;An algorithm is proposed to convert the coarse-grained A-graft-B model of polylactic acid into the atomistic representation. In the A-graft-B model the atoms of the backbone are mapped onto A beads, which form the linear backbone of the coarse-grained macromolecule, the methyl groups are mapped onto B side pendants. The algorithm restores atomic positions based on positions of coarse-grained beads with the help of pre-defined chain fragments, called templates. The dimensions of the templates are adjusted by affine transformation to ensure coincidence of the backbone in coarse-grained and atomistic representation. The transition between coarse-grained and atomistic models conserves information about the fine structure of polymer chains. The restored configurations are suitable for further molecular-dynamic simulations. Both atomistic and coarse-grained representations require standard GROMACS software. The algorithm can be used for reverse mapping of other A-graft-B polymer models.
10.14529/jsfi180320;Supercomputer Technologies as a Tool for Highresolution Atmospheric Modelling towards the Climatological Timescales;Estimation of the recent and future climate changes is the most important challenge in the modern Earth sciences. Numerical climate models are an essential tool in this field of research. However, modelling results are highly sensitive to the spatial resolution of the model. The most of the climate change studies utilize the global atmospheric models with a grid cell size of tens of kilometres or more. High-resolution mesoscale models are much more detailed, but require significantly more computational resources. Applications of such high-resolution models in climate studies are usually limited by regional simulations and by relatively short timespan. In this paper we consider the experience of the long-term regional climate studies based on the mesoscale modelling. On the examples of urban climate studies and extreme wind assessments, we demonstrate the principle advantage of long-term high-resolution simulations, which were carried out on the modern supercomputers.
10.14529/jsfi180408;Supercomputer Simulations of FluidStructure Interaction Problems Using an Immersed Boundary Method;The paper describes a supercomputer application in simulations of fluid-structure interaction problems. A compressible flow solver based on a high-accuracy scheme for unstructured hybrid meshes is considered. It combines an immersed boundary method with a dynamic mesh adaptation method in order to represent motion of solid objects in a turbulent flow. The use of immersed boundaries allows you to dynamically adapt the mesh resolution near moving solid surfaces without changing the mesh topology. Multilevel MPI + OpenMP parallelization of these components fits well with the architecture of modern cluster systems. The proposed implementation can engage thousands of CPU cores in one simulation efficiently. An example application is presented in which a high-speed turbulent flow around a cavity with a deflector is simulated.
10.14529/jsfi180409;Test of Computational Approaches for GoldThiolate Clusters Calculation;High-level procedures (MP2, CCSD, CCSD(T)) and reliable experimental data have been used to assess the performance of a variety of exchange-correlation functionals for the calculation of structures and energies of small models of thiolate-protected gold clusters. Clusters represent rather complicated objects for examination, therefore the simple models including Au2, AuS were considered to find an appropriate method to calculate Au-Au and Au-S interactions in protected clusters. The mean unsigned errors of the quantum chemical methods were evaluated via reliable experimental bond distances and dissociation energies of Au2 and AuS. Based on the calculation, the SVWN5, TPSS+D3, PBE96+D3, and PBE0+D3 were found to give the most reliable results and can be recommended for calculation of the structure and properties of thiolate-protected gold clusters. The influence of the relativistic corrections calculated in Dirac-Coulomb-Breit framework and inclusion of dispersion corrections on the structure and energy of thiolate-protected gold clusters have been analyzed.
10.14529/jsfi180410;Supercomputer Modeling of DualSite Acetylcholinesterase AChE Inhibition;Molecular docking is one of the most popular tools of molecular modeling. However, in certain cases, like development of inhibitors of cholinesterases as therapeutic agents for Alzheimer's disease, there are many aspects, which should be taken into account to achieve accurate docking results. For simple molecular docking with popular software and standard protocols, a personal computer is sucient, however quite often the results are irrelevant. Due to the complex biochemistry and biophysics of cholinesterases, computational research should be supported with quantum mechanics (QM) and molecular dynamics (MD) calculations, what requires the use of supercomputers. Experimental studies of inhibition kinetics can discriminate between dierent types of inhibition-competitive, non-competitive or mixed type-that is quite helpful for assessment of the docking results. Here we consider inhibition of human acetylcholinesterase (AChE) by the conjugate of MB and 2,8-dimethyl-tetrahydro-y-carboline, study its interactions with AChE in relation to the experimental data, and use it as an example to elucidate crucial points for reliable docking studies of bulky AChE inhibitors. Molecular docking results were found to be extremely sensitive to the choice of the X-ray AChE structure for the docking target and the scheme selected for the distribution of partial atomic charges. It was demonstrated that exible docking should be used with an additional caution, because certain protein conformational changes might not correspond with available X-ray and MD data.
10.14529/jsfi180321;Supercomputer Simulations in Design of Ultrasound Tomography Devices;The paper considers the use of supercomputers in design of medical ultrasound tomography devices. The mathematical models describing the wave propagation in ultrasound tomography should take into account such physical phenomena as diffraction, multiple scattering, and so on. The inverse problem of wave tomography is posed as a coefficient inverse problem with respect to the wave propagation velocity and the absorption factor. Numerous simulations made it possible to determine the optimal parameters of an ultrasound tomograph in order to obtain a spatial resolution of 1.5 mm suitable for early-stage breast cancer diagnosis. The developed methods were tested both on model problems and on real data obtained at the experimental test bench for tomographic studies. The computations were performed on GPU devices of Lomonosov-2 supercomputer at Lomonosov Moscow State University.
10.14529/jsfi190101;Efficient Parallel Implementation of MultiArrival 3D Prestack Seismic Depth Migration;The goal of seismic migration is to reconstruct the image of Earth's depth inhomogeneities on the base of seismic data. Seismic data is obtained using shots in shallow wells that are located in a dense grid points. Those shots could be considered as special point sources. A reflected and scattered seismic waves from the depth inhomogeneities are received by geophones located also in a dense grid points on a surface. A seismic image of depth inhomogeneities can be constructed based on these waves. The implementation of 3-D seismic migration implies the solution of about 104ÐÂ·5 3-D direct problems of wave propagation. Hence efficient asymptotic methods are of a great practical importance. The multi-arrival 3-D seismic migration program is implemented based on a new asymptotic method. It takes into account multi-pass wave propagation and caustics. The program uses parallel calculations in an MPI environment on hundreds and thousands of processor cores. The program was successfully tested on an international synthetic SEG salt data set and on real data. A seismic image cube for Timan-Pechora region is given as an example.
10.14529/jsfi180407;New Binding Mode of SLURP Protein to a7 Nicotinic Acetylcholine Receptor Revealed by Computer Simulations; SLURP-1 is a member of three-finger toxin-like proteins. Their characteristic feature is a set of three beta strands extruding from hydrophobic core stabilized by disulfide bonds. Each beta-strand carries a flexible loop, which is responsible for recognition. SLURP-1 was recently shown to act as an endogenous growth regulator of keratinocytes and tumor suppressor by reducing cell migration and invasion by antagonizing the pro-malignant effects of nicotine. This effect is achieved through allosteric interaction with alpha7 nicotinic acetylcholine receptors (alpha-7 nAChRs) in an antagonist-like manner. Moreover, this interaction is unaffected by several well-known agents specifically alpha-bungarotoxin.In this work, we carry out the conformational analysis of the SLURP-1 by a microsecond-long full-atom explicit solvent molecular dynamics simulations followed by clustering, to identify representative states. To achieve this timescale we employed a GPU-accelerated version of GROMACS modeling package. To avoid human bias in clustering we used a non-parametric clustering algorithm Affinity Propagation adapted for biomolecules and HPC environments. Then, we applied protein-protein molecular docking of the ten most massive clusters to alpha7-nAChRs in order to test if structural variability can affect binding. Docking simulations revealed the unusual binding mode of one of the minor SLURP-1 conformations.  
10.14529/jsfi180411;Supercomputer Simulations of DopamineDerived Ligands Complexed with Cyclooxygenases;An in silico approach was adopted to identify potential cyclooxygenase inhibitors through molecular docking studies. Four potentially active molecules were generated by fusion of dopamine with ibuprofen or ketorolac derivatives. The binding mode of the considered ligands to cyclooxygenase-1 and cyclooxygenase-2 isoforms was described using Autodock Vina. Preliminary docking to full cyclooxygenase isoforms' structures was used to determine possible binding sites for the described dopamine-derived ligands. The following more accurate docking iteration to the described binding sites was used to achieve better conformational sampling. Among the studied molecules, IBU-GABA-DA showed preferable binding to cyclooxygenase active site of cyclooxygenase-1, while IBU-DA bound to peroxidase site of cyclooxygenase-1, making these ibuprofen-comprising ligands a base for further research and design of selective cyclooxygenase-1 inhibitors. Keterolac-derived ligands KET-DA and KET-GABA-DA demonstrated binding to both cyclooxygenase isoforms at a side pocket, which does not relate to any known functional site of cyclooxygenases and needs to be further investigated.
10.14529/jsfi180412;High Performance Computing of Magnetized Galactic Disks;A parallel implementation of the magneto-hydrodynamical code for global modeling of the galactic evolution is reported. The code is parallelized by using MPI interface, and it shows ideal scaling up to 200300 cores on Lomonosov supercomputer with fast interconnect. In the benchmarking of this code, we study the dynamics of a magnetized gaseous disk of a galaxy with a bar. We run a high-resolution 3D magnetohydrodynamic simulation taking into account the Milky Way-like gravitational potential, gas self-gravity and a network of cooling and heating processes in the interstellar medium. By using this simulation the evolution of morphology and enhancement of the magnetic field are explored. In agreement to hydrodynamical models, when the bar is strong enough, the gas develops sharp shocks at the leading side of the bar. In such a picture we found that when typically the magnetic field strength traces the location of the largescale shocks along the bar major axis, the magnetic field pressure weakens the shocks and reduces the inflow of gas towards the galactic center.
10.14529/jsfi180413;Regional Climate Model for the Lower Volga: Parallelization Efficiency Estimation;We have deployed the regional climate model (RCM) RegCM 4.5 for the Lower Volga and adjacent territories with a horizontal spatial resolution of 20 km. The problems of choosing the computational domain in the RCM RegCM version 4.5 are considered. We demonstrate the influence of this factor on the forecast of rainfall distribution in the numerical simulations. The study of rainfall and snowfall is a more demanding test in comparison with temperature or pressure distributions. We investigate dependencies of calculation time, parallel speedup and parallelization efficiency on the number of processes for different multi-core CPUs. Our analysis of the efficiency of parallel implementation of RegCM for various multi-core and multi-processor systems show a strong dependence of the simulation speed on the CPU type. The best effect is achieved when the number of CPU threads and the number of parallel processes are equal. The parallel code speedup is in the range of 1.8 - 11 for different CPUs.
10.14529/jsfi180322;Recent Progress on Supercomputer Modelling of HighSpeed Rarefied Gas Flows Using Kinetic Equations;Numerical solution of the Boltzmann equation for stationary high-speed flows around complex three-dimensional bodies is an extremely difficult computational problem. This is because of high dimension of the equation and lack of efficient implicit methods for the calculation of the collision integral on arbitrary non-uniform velocity grids. Therefore, the use of the so-called model (approximate) kinetic equations appears to be more appropriate and attractive. This article uses the numerical methodology recently developed by the second author which includes an implicit method for solving the approximating kinetic equation of E.M. Shakhov (S-model) on arbitrary unstructured grids in both velocity and physical spaces. Since most of model equations have a well-known drawback associated with the velocityindependent collision frequency it is important to determine the deviations of solutions of these equations from the solution of the complete Boltzmann equation or DSMC for high-speed gas flows. Our recent comparison of the DSMC and S-model solutions for monatomic gases with a soft interaction potential shows good agreement of surface coefficients of the pressure, heat transfer and friction, which are most important for industrial applications. In this paper, we compare the solution of model equations and the Boltzmann equation for the problem of supersonic gas flow around a cylinder when molecules interact according to the law of hard spheres. Since this law of molecular interaction is the most rigid, the difference in solutions can show the maximum error that can be obtained by using model equations instead of the exact Boltzmann equation in such problems. Our high-fidelity computations show that the use of model kinetic equations with adaptation in phase space is very promising for industrial applications.
10.14529/jsfi190102;LAMMPS Code Simulation of the Defect Formation Induced by Ion Incidence in Carbon Nanotubes;A molecular dynamic calculation of the multi-walled carbon nanotube thermal sputtering induced by ion irradiation is carried out. Sputtering results comparable to experimental data are obtained. There are two models of ion and thermal sputtering discussed in the paper. The simulation tested the model of thermal amorphization and revealed that the disordering of multi-walled carbon nanotubes structure occurs as a result of their heating under ion irradiation. Classical molecular dynamic simulation was performed using LAMMPS code. Simulation cell with 14 layers multi-walled carbon nanotube 121230 nm size contains 285600 atoms. Multi-walled carbon nanotube was irradiated by 80 keV energy Ar+ ions in cumulative mode. Simulation was performed on the Lomonosov-1 supercomputer. About 24600 nodes-hours were spent on one simulation as a whole. The balancing of MPI ows for a spatial grid of counting nodes occurred according to the scheme 88128 MPI-stream. LAMMPS code was built with Intel 12.0 compiler. This configuration allowed to speed up the calculation in comparison with the calculation on a single-processor Xeon CPU X5570 2.93 GHz machine by 60 times.
10.14529/jsfi180323;Supercomputer Modeling of Parachute Flight Dynamics;In this article the authors present parallel implementation of numerical method for computer modeling of dynamics of a parachute with filled canopy. To solve the 3D problem of parachute free motion numerically, authors formulate tied problem of dynamics and aerodynamics where aerodynamic characteristics are found with discrete vortices method on each step of integration in time, and to find motion law the corresponding motion equations have to be solved. The solution of such problems requires high computational resources because it is important to model parachute motion during a long physical time period. Herewith the behavior of vortex wake behind the parachute is important and has to be modeled. In the approach applied by the authors the wake is modeled as a set of flexible vortex elements. So to increase computational efficiency, the authors used methods of low-rank matrix approximations, as well as parallel implementations of algorithms. Short description of numerical method is presented, as well as the examples of numerical modeling.
10.14529/jsfi180324;Supercomputer Simulation of MATISH Problem;A supercomputer simulation of the benchmark MATiS-H problem is considered. A highresolution CABARET code is applied for solving Navier-Stokes equations in the framework of the Monotonically Integrated LES approach for the MATiS-H problem. The code is based on a generalisation of low-dissipative, low-dispersive and non-oscillatory CABARET scheme to hybrid topology meshes in the supercomputing framework. The solutions for the time-averaged fields are reported. These show a relatively small sensitivity to the grid density. Comparison with the experiment data available is provided.
10.14529/jsfi180414;Performance Analysis of Different Computational Architectures: Molecular Dynamics in Application to Protein Assemblies Illustrated by Microtubule and Electron Transfer Proteins;"All-atom molecular dynamics simulation represents a computationally challenging, but powerful approach for studying conformational changes and interactions of biomolecules and their assemblies of different kinds. Usually, the numbers of simulated particles in modern molecular dynamics studies range from thousands to tens of millions, while the simulated timescales span from nanoseconds to microseconds.  For cost and computation efficiency, it is important to determine the optimal computer hardware for simulations of biomolecular systems of different size and timescale. Here we compare performance and scalability of 17 commercially available computational architectures, using molecular dynamics simulations of water and two different protein systems in GROMACS-5 package as computing benchmarks. We report typical single-node performance of various combinations of modern CPUs and GPUs, as well as multiple-node performance of ""Lomonosov-2"" supercomputer in molecular dynamics simulations of different protein systems in nanoseconds per day. These data can be used as practical guidelines for selection of optimal computer hardware for various molecular dynamics simulation tasks. "
10.14529/jsfi180325;Highperformance Fullatomistic Simulation of Optical Thin Films;The experimental study of the dependence of thin film properties on the deposition conditions may be still a great challenge. Today the progress in high performance computing allows one to perform the investigation of these dependencies on the atomistic level using the classical molecular dynamics (MD) simulation. In the present work the computational cost and efficiency of classical full-atomistic simulation of thin film deposition process using the Lonmonosov-2 supercomputer facilities is discussed. It is demonstrated that using 512 computational cores of the Lomonosov-2 supercomputer ensures the simulation of thin film cluster with technologically meaningful thickness of an optical film. Because of a relatively slow growth of the simulation time with the increase of film thickness we guess that simulations clusters with thicknesses that are several times higher than the currently achieved thicknesses about one hundred nanometers is quite realistic if the number of available computational cores will be increased up to several thousands.
10.14529/jsfi180415;Algorithm of the Parallel Sweep Method for Numerical Solution of the Gross-Pitaevskii Equation with Highest Nonlinearities;In this paper, we for the first time introduce a numerical scheme the solution of a nonlinear equation of the Gross-Pitaevskii type (GP) or the nonlinear Schrodinger equation (NLSE) with highest nonlinearities, which provides implementation of a complete set of motion integrals. This scheme was parallelly implemented on a non-uniform grid. Propagation of a ring laser beam with non-zero angular momentum in the filamentation mode is studied using the implemented numerical scheme. It is shown, that filaments under exposure to centrifugal forces escape to the periphery. Based on a number of numerical experiments, we have found the universal property of motion integrals in the non-conservative case for a given class of equations. Research of dynamics of angular momentum for a dissipative case are also presented. We found, that angular moment, particularly normed by initial energy during filamentation process, is quasi-constant.
10.14529/jsfi190103;A Fully Conservative Parallel Numerical Algorithm with Adaptive Spatial Grid for Solving Nonlinear Diffusion Equations in Image Processing;In this paper we present simple yet efficient parallel program implementation of grid-difference method for solving nonlinear parabolic equations, which satisfies both fully conservative property and second order of approximation on non-uniform spatial grid according to geometrical sanity of a task. The proposed algorithm was tested on Perona-Malik method for image noise ltering task based on differential equations. Also in this work we propose generalization of the Perona-Malik equation, which is a one of diffusion in complex-valued region type. This corresponds to the conversion to such types of nonlinear equations like Leontovich-Fock equation with a dependent on the gradient field according to the nonlinear law coefficient of diffraction. This is a special case of generalization of the Perona-Malik equation to the multicomponent case. This approach makes noise removal process more flexible by increasing its capabilities, which allows achieving better results for the task of image denoising.
10.14529/jsfi180326;Supercomputer Docking: Investigation of Low Energy Minima of ProteinLigand Complexes;It is shown that the global energy minimum of a protein-ligand complex, when the energy is calculated by the PM7 quantum-chemical semiempirical method with the COSMO implicit solvent model, can be determined as follows. First, the low energy minima are found by a docking program when the protein-ligand energy is calculated with the MMFF94 force field in vacuum. Second, energies of all these minima are recalculated with the PM7 method and the COSMO implicit solvent model. Third, among these recalculated energies the minimal energy is determined and the respective minimum is the global energy minimum when the energy is calculated with the PM7 method and the COSMO implicit solvent model. The optimal width of the spectrum of low energy minima found with MMFF94 in vacuum is determined to perform minimal quantity of quantum-chemical recalculations. The proposed approach allows to perform docking in solvent with the quantum-chemical method and to increase the docking positioning accuracy.
10.14529/jsfi190104;Parametrization of the Elastic Network Model Using HighThroughput Parallel Molecular Dynamics Simulations;Even when modern computational platforms and parallel techniques are used, conventional all-atom simulations are limited both in terms of reachable timescale and number of atoms in the biomolecular system of interest. On the other hand, coarse-grained models, which allow to overcome this limitation, rely on proper and rigorous parametrization of the underlying force field. Here, we present a novel iterative approach for parametrization of coarse-grained models based on direct comparison of equilibrium simulations at all-atom and coarse-grained resolutions. In order to assess the accuracy of our method, we have built and parametrized an elastic network model (ENM) of the tubulin protolament consisting of four monomers. For this system, our method shows good convergence and the parametrized ENM reproduces protein dynamics in a finer way when compared to ENMs parametrized using the conventional approach. The presented method can be extended to other coarse-grained models with a slight adjustment of the equations describing the iterative scheme.
10.14529/jsfi180406;Developing QuasiSteady Model for Studying Hemostatic Response Using Supercomputer Technologies;Formation of the platelet plug represents a primary response to the vessel wall injury, but may also result in vessel occlusion. The decrease of the local blood flow due to platelet thrombus formation may lead to serious complications, such as ischemic stroke and myocardial infarction. However, mechanisms responsible for regulation of thrombus dynamics are not clear. In order to get a deeper insight into the role of blood flow and platelet interactions in the formation of the primary platelet plug we developed a particle-based model of microvascular thrombosis using quasisteady flow approximation. In order to simulate thrombus dynamics at physiologically relevant timescales of several minutes, we took advantage of the supercomputer technologies. Our in silico analysis revealed the importance of platelet size heterogeneity for describing experimental data on microvascular thrombus formation. Thus, our model represents a useful tool for the supercomputeraided computational analysis of thrombus dynamics in the microvessels on physiologically relevant timescales.
10.14529/jsfi190106;Performance Evaluation of Different Implementation Schemes of an Iterative Flow Solver on Modern Vector Machines;Modern supercomputers consist of multi-core processors, and these processors have recently employed vector instructions, or so-called SIMD instructions, to improve performances. Numerical simulations need to be vectorized in order to achieve higher performance on these processors. Various legacy numerical simulation codes that have been utilized for a long time often contain two versions of source codes: a non-vectorized version and a vectorized version that is optimized for old vector supercomputers. It is important to clarify which version is better for modern supercomputers in order to achieve higher performance. In this paper, we evaluate the performances of a legacy fluid dynamics simulation code called FASTEST on modern supercomputers in order to provide a guidepost for migrating such codes to modern supercomputers. The solver has a nonvectorized version and a vectorized version, and the latter uses the hyperplane ordering method for vectorization. For the evaluation, we also implement the red-black ordering method, which is another way to vectorize the solver. Then, we examine the performance on NEC SX-ACE, SXAurora TSUBASA, Intel Xeon Gold, and Xeon Phi. The results show that the shortest execution times are with the red-black ordering method on SX-ACE and SX-Aurora TSUBASA, and with the non-vectorized version on Xeon Gold and Xeon Phi. Therefore, achieving a higher performance on multiple modern supercomputers potentially requires maintenance of multiple code versions. We also show that the red-black ordering method is more promising to achieve high performance on modern supercomputers.
10.14529/jsfi190202;HPC Processors Benchmarking Assessment for Global System Science Applications;The work undertaken in this paper was done in the Centre of Excellence for Global Systems Science (CoeGSS) - an interdisciplinary project funded by the European Commission. CoeGSS project provides a computer-aided decision support in the face of global challenges (e.g. development of energy, water and food supply systems, urbanisation processes and growth of the cities, pandemic control, etc.) and tries to bring together HPC and global systems science. This paper presents a proposition of GSS benchmark which evaluates HPC architectures with respect to GSS applications and seeks for the best HPC system for typical GSS software environments. The outcome of the analysis is defining a benchmark which represents the average GSS environment and its challenges in a good way: spread of smoking habits and development of tobacco industry, development of green cars market and global urbanisation processes. Results of the tests that have been run on a number of recently appeared HPC platforms allow comparing processors' architectures with respect to different applications using execution times, TDPs3 and TCOs4 as the basic metrics for ranking HPC architectures. Finally, we believe that our analysis of the results conveys a valuable information to the broadened GSS audience which might help to determine the hardware demands for their specific applications, as well as to the HPC community which requires a mature benchmark set reflecting requirements and traits of the GSS applications. Our work can be considered as a step into direction of development of such mature benchmark.
10.14529/jsfi190107;Comparative Analysis of Virtualization Methods in Big Data Processing;Cloud computing systems have become widely used for Big Data processing, providing access to a wide variety of computing resources and a greater distribution between multi-clouds. This trend has been strengthened by the rapid development of the Internet of Things (IoT) concept. Virtualization via virtual machines and containers is a traditional way of organization of cloud computing infrastructure. Containerization technology provides a lightweight virtual runtime environment. In addition to the advantages of traditional virtual machines in terms of size and flexibility, containers are particularly important for integration tasks for PaaS solutions, such as application packaging and service orchestration. In this paper, we overview the current state-of-the-art of virtualization and containerization approaches and technologies in the context of Big Data tasks solution. We present the results of studies which compare the efficiency of containerization and virtualization technologies to solve Big Data problems. We also analyze containerized and virtualized services collaboration solutions to support automation of the deployment and execution of Big Data applications in the cloud infrastructure.
10.14529/jsfi190203;How Fileaccess Patterns Influence the Degree of I/O Interference between Cluster Applications;On large-scale clusters, tens to hundreds of applications can simultaneously access a parallel file system, leading to contention and, in its wake, to degraded application performance. In this article, we analyze the influence of file-access patterns on the degree of interference. As it is by experience most intrusive, we focus our attention on write-write contention. We observe considerable differences among the interference potentials of several typical write patterns. In particular, we found that if one parallel program writes large output files while another one writes small checkpointing files, then the latter is slowed down when the checkpointing files are small enough and the former is vice versa. Moreover, applications with a few processes writing large output files already can significantly hinder applications with many processes from checkpointing small files. Such effects can seriously impact the runtime of real applications-up to a factor of five in one instance. Our insights and measurement techniques offer an opportunity to automatically classify the interference potential between applications and to adjust scheduling decisions accordingly.
10.14529/jsfi190204;Investigating the Dirac Operator Evaluation with FPGAs;In recent years, computational capacity of single Field Programmable Gate Array (FPGA) devices as well as their versatility have increased significantly. Adding to that fact, the High Level Synthesis frameworks allowing to program such processors in a high-level language like C++, makes modern FPGA devices a serious candidate as building blocks of a general-purpose High Performance Computing solution. In this contribution we describe benchmarks which we performed using a kernel from the Lattice QCD code, a highly compute-demanding HPC academic code for elementary particle simulations on the newest device from Xilinx, the U250 accelerator card. We describe the architecture of our solution and benchmark its performance on a single FPGA device running in two modes: using either external or embedded memory. We discuss both approaches in detail and provide assessment for the necessary memory throughput and the minimal amount of resources needed to deliver optimal performance depending on the available hardware. Our considerations can be used as guidelines for estimating the performance of some larger, manynode systems.
10.14529/jsfi190205;Development of a RISCVConform Fused MultiplyAdd FloatingPoint Unit;Despite the fact that the open-source community around the RISC-V instruction set architecture is growing rapidly, there is still no high-speed open-source hardware implementation of the IEEE 754-2008 floating-point standard available. We designed a Fused Multiply-Add Floating-Point Unit compatible with the RISC-V ISA in SystemVerilog, which enables us to conduct detailed optimizations where necessary. The design has been verified with the industry standard simulation-based Universal Verification Methodology using the Specman e Hardware Verification Language. The most challenging part of the verification is the reference model, for which we integrated the Floating-Point Unit of an existing Intel processor using the Function Level Interface provided by Specman e. With the use of Intel's Floating-Point Unit we have a known good and fast reference model. The Back-End flow was done with Global Foundries' 22 nm Fully-Depleted Silicon-On-Insulator (GF22FDX) process using Cadence tools. We reached 1.8 GHz over PVT corners with a 0.8 V forward body bias
10.14529/jsfi190206;Fully Implicit Time Stepping Can Be Efficient on Parallel Computers;Benchmarks in high performance computing often involve a single component used in the full solution of a computational problem, such as the solution of a linear system of equations. In many cases, the choice of algorithm, which can determine the components used, is also important when solving a full problem. Numerical evidence suggests that for the Taylor-Green vortex problem at a Reynolds number of 1600, a second order implicit midpoint rule method can require less computational time than the often used linearly implicit Carpenter-Kennedy method for solving the equations of incompressible fluid dynamics for moderate levels of accuracy at the beginning of the flow evolution. The primary reason is that even though the implicit midpoint rule is fully implicit, it can use a small number of iterations per time step, and thus require less computational work per time step than the Carpenter-Kennedy method. For the same number of timesteps, the Carpenter-Kennedy method is more accurate since it uses a higher order timestepping method.
10.14529/jsfi190207;Performance Limits Study of Stencil Codes on Modern GPGPUs;We study the performance limits of different algorithmic approaches to the implementation of a sample problem of wave equation solution with a cross stencil scheme. With this, we aim to find the highest limit of the achievable performance efficiency for stencil computing.To estimate the limits, we use a quantitative Roofline model to make a thorough analysis of the performance bottlenecks and develop the model further to account for the latency of different levels of GPU memory. These estimates provide an incentive to use spatial and temporal blocking algorithms. Thus, we study stepwise, domain decomposition, and domain decomposition with halo algorithms in that order. The knowledge of the limit incites the motivation to optimize the implementation. This led to the analysis of the block synchronization methods in CUDA, which is also provided in the text.  After all optimizations, we have achieved 90% of the peak performance, which amounts to more than 1 trillion cell updates per second on one consumer level GPU device.
10.14529/jsfi190208;Distinct Element Simulation of Mechanical Properties of Hypothetical CNT Nanofabrics;A universal framework for modeling composites and fabrics of micro- and nanofibers, such as carbon nanotubes, carbon fibers and amyloid fibrils, is presented. Within this framework, fibers are represented with chains of rigid bodies, linked with elastic bonds. Elasticity of the bonds utilizes recently developed enhanced vector model formalism. The type of interactions between fibers is determined by their nature and physical length scale of the simulation. The dynamics of fibers is computed using the modification of rigid particle dynamics module of the waLBerla multiphysics framework. Our modeling system demonstrates exceptionally high parallel performance combined with the physical accuracy of the modeling. The efficiency of our technique is demonstrated with an illustrative mechanical test on a hypothetical carbon nanotube textile. In this example, the elasticity of the fibers represents the coarse-grained covalent bond within CNT surface, whereas interfiber interactions represent coarse-grained van der Waals forces between cylindrical segments of nanotubes. Numerical simulation demonstrates stability and extremal strength of a hypothetical carbon nanotube fabric.
10.14529/jsfi190201;Supercomputer Lomonosov2: Large Scale Deep Monitoring and Fine Analytics for the User Community;The huge number of hardware and software components, together with a large number of parameters affecting the performance of each parallel application, makes ensuring the efficiency of a large scale supercomputer extremely difficult. In this situation, all basic parameters of the supercomputer should be constantly monitored, as well as many decisions about its functioning should be made by special software automatically. In this paper we describe the tight connection between complexity of modern large high performance computing systems and special techniques and tools required to ensure their efficiency in practice. The main subsystems of the developed complex (Octoshell, DiMMoN, Octotron, JobDigest, and an expert software system to bring fine analytics on parallel applications and the entire supercomputer to users and sysadmins) are actively operated on the large supercomputer systems at Lomonosov Moscow State University. A brief description of the architecture of Lomonosov-2 supercomputer is presented, and questions showing both a wide variety of emerging complex issues and the need for an integrated approach to solving the problem of effectively supporting large supercomputer systems are discussed.
10.14529/jsfi190401;An Energyaware Dynamic Data Allocation Mechanism for Manychannel Memory Systems;A modern memory system is equipped with many memory channels to obtain a high memory bandwidth. To take the advantage of this organization, applications' data are distributed among the channels and transferred in an interleaved fashion. Although memory-intensive applications benefit from a high bandwidth by many memory channels, applications such as compute-intensive ones do not need the high bandwidth. To reduce the energy consumption for such applications, the memory system has low-power modes. During no memory request, the main memory can enter these modes and reduce energy consumption. However, these applications often cause intermittent memory requests to the channels that handle their data, resulting in not entering the low-power modes. Hence, the memory system cannot enter the low-power modes even though the applications do not need the high bandwidth. To solve this problem, this paper proposes a dynamic data allocation mechanism for many-channel memory systems. This mechanism forces data of such applications to use the specified channels by dynamically changing the address-mapping schemes and migrating the data. As a result, the other channels to which the data are not allocated can have a chance to enter the low-power modes for a long time. Therefore, the proposed mechanism has the potential to reduce the energy consumption of many-channel memory systems. The evaluation results show that this mechanism can reduce the energy consumption by up to 11.8% and 1.7% on average.
10.14529/jsfi190301;Collecting and Presenting Reproducible Intranode Stencil Performance: INSPECT;Stencil algorithms have been receiving considerable interest in HPC research for decades. The techniques used to approach multi-core stencil performance modeling and engineering span basic runtime measurements, elaborate performance models, detailed hardware counter analysis, and thorough scaling behavior evaluation. Due to the plurality of approaches and stencil patterns, we set out to develop a generalizable methodology for reproducible measurements accompanied by state-of-the-art performance models. Our open-source toolchain and collected results are publicly available in the Intranode Stencil Performance Evaluation Collection (INSPECT). We present the underlying methods, models and tools involved in gathering and documenting the performance behavior of a collection of typical stencil patterns across multiple architectures and hardware configuration options. Our aim is to endow performance-aware application developers with reproducible baseline performance data and validated models to initiate a well-defined process of performance assessment and optimization. All data is available for inspection: source code, produced assembly, performance measurements, hardware performance counter data, single-core and multicore Roofline and ECM (execution-cache-memory) performance models, and machine properties. Deviations between measured performance and performance models become immediately evident and can be investigated. We also give hints as to how INSPECT can be used in practice for custom code analysis.
10.14529/jsfi190402;Towards Heterogeneous Multiscale Computing on Large Scale Parallel Supercomputers;New applications that can exploit emerging exascale computing resources efficiently, while providing meaningful scientific results, are eagerly anticipated. Multi-scale models, especially multi-scale applications, will assuredly run at the exascale. We have established that a class of multi-scale applications implementing the heterogeneous multi-scale model follows, a heterogeneous multi-scale computing (HMC) pattern, which typically features a macroscopic model synchronising numerous independent microscopic model simulations. Consequently, communication between microscopic simulations is limited. Furthermore, a surrogate model can often be introduced between macro-scale and micro-scale models to interpolate required data from previously computed micro-scale simulations, thereby substantially reducing the number of micro-scale simulations. Nonetheless, HMC applications, though versatile, remain constrained by load balancing issues. We discuss two main issues: the a priori unknown and variable execution time of microscopic simulations, and the dynamic number of micro-scale simulations required. We tackle execution time variability using a pilot job mechanism to handle internal queuing and multiple sub-model execution on large-scale supercomputers, together with a data-informed execution time prediction model. To dynamically select the number of micro-scale simulations, the HMC pattern automatically detects and identifies three surrogate model phases that help control the available and used core amount. After relevant phase detection and micro-scale simulation scheduling, any idle cores can be used for surrogate model update or for processor release back to the system. We demonstrate HMC performance by testing it on two representative multi-scale applications. We conclude that, considering the subtle interplay between the macroscale model, surrogate models and micro-scale simulations, HMC provides a promising path towards exascale for many multiscale applications.
10.14529/jsfi190302;Supercomputer Docking;This review is based on the peer-reviewed research literature including the author's own publications devoted to supercomputer docking. The general view on docking and its role at the initial stage of the rational drug design is presented. Molecules of medicine compounds selectively bind to the active site of a protein, which is responsible for the disease progression, and stop it. Docking programs perform positioning of molecules (ligands) in the active site of the protein and estimate the protein-ligand binding energy. The larger this energy is, the less concentration of the respective compound should be used to observe the desired effect. Several classical docking programs are described in short. Examples of the adaptation of existing docking programs to supercomputing and using them for virtual screening of millions of ligands are presented. Two novel generalized docking programs specially designed for multi-core docking of a single ligand on a supercomputer are described shortly. These programs find a sufficiently wide spectrum of low energy minima of a protein-ligand complex in the frame of a given force field. The quasi-docking procedure using the generalized docking program is described. Quasi-docking allows to perform docking with quantum-chemical semiempirical methods. Finally a summary is made based on the materials presented.
10.14529/jsfi190303;Automatic Port to OpenACC/OpenMP for Physical Parameterization in Climate and Weather Code Using the CLAW Compiler;In order to benefit from emerging high-performance computing systems, weather and climate models need to be adapted to run efficiently on different hardware architectures such as accelerators. This is a major challenge for existing community models that represent extremely large codebase written in Fortran. Large parts of the code can be ported using OpenACC compiler directives but for time-critical components such as physical parameterizations, code restructuring and optimizations specific to a hardware architecture are necessary to obtain high performance. In an effort to retain a single source code for multiple target architectures, the CLAW Compiler and the CLAW Single Column Abstraction were introduced. We report on the extension of the CLAW SCA to handle ELEMENTAL functions and subroutines. We demonstrate the new capability on the JSBACH land surface scheme of the ICON climate model. With the extension, JSBACH can be automatically ported to OpenACC or OpenMP for accelerators with minimal to no change to the original code.
10.14529/jsfi190304;Optimizing Deep Learning RNN Topologies on Intel Architecture;Recurrent neural network (RNN) models have been found to be well suited for processing temporal data. In this work, we present an optimized implementation of vanilla RNN cell and its two popular variants: LSTM and GRU for Intel Xeon architecture. Typical implementations of these RNN cells employ one or two large matrix multiplication (GEMM) calls and then apply the element-wise operations (sigmoid/tanh) onto the GEMM results. While this approach is easy to implement by exploiting vendor-optimized GEMM library calls, the data reuse relies on how GEMMs are parallelized and is sub-optimal for GEMM sizes stemming from small minibatch. Also, the element-wise operations are exposed as a bandwidth-bound kernel after the GEMM which is typically a compute-bound kernel. To address this discrepancy, we implemented a parallel blocked matrix GEMM in order to (a) achieve load balance, (b) maximize weight matrix reuse, (c) fuse the element-wise operations after partial GEMM blocks are computed and while they are hot in cache. Additionally, we bring the time step loop in our cell to further increase the weight reuse and amortize the overhead to transform the weights into blocked layout. The results show that our implementation is generally faster than Intel MKL-DNN library implementations, e.g. for RNN, forward pass is up to ~3 faster whereas the backward/weight update pass is up to ~5 faster. Furthermore, we investigate high-performance implementations of sigmoid and tanh activation functions that achieve various levels of accuracy. These implementations rely on minimax polynomial approximations, rational polynomials, Taylor expansions and exponential approximation techniques. Our vectorized implementations can be flexibly integrated into deep learning computations with different accuracy requirements without compromising performance
10.14529/jsfi190305;A Skewed Multibanked Cache for Manycore Vector Processors;As the number of cores and the memory bandwidth have increased in a balanced fashion, modern vector processors achieve high sustained performances, especially in memory-intensive applications in the fields of science and engineering. However, it is difficult to significantly increase the off-chip memory bandwidth owing to the limitation of the number of input/output pins integrated on a single chip. Under the circumstances, modern vector processors have adopted a shared cache to realize a high sustained memory bandwidth. The shared cache can effectively reduce the pressure to the off-chip memory bandwidth by keeping reusable data that multiple vector cores require. However, as the number of vector cores sharing a cache increases, more different blocks requested from multiple cores simultaneously use the same set. As a result, conflict misses caused by these blocks degrade the performance.In order to avoid increasing the conflict misses in the case of the increasing number of cores, this paper proposes a skewed cache for many-core vector processors. The skewed cache prevents the simultaneously requested blocks from being stored into the same set. This paper discusses how the most important two features of the skewed cache should be implemented in modern vector processors: hashing function and replacement policy. The proposed cache adopts the oddmultiplier displacement hashing for effective skewing and the static re-reference interval prediction policy for reasonable replacing. The evaluation results show that the proposed cache significantly improves the performance of a many-core vector processor by eliminating conflict misses.
10.14529/jsfi190403;Improving Reliability of Supercomputer CFD Codes on Unstructured Meshes;"The paper describes a particular technical solution targeted at improving reliability and quality of a highly-parallel computational fluid dynamics code written in C++. The code considered is based on rather complex high-accuracy numerical methods and models for simulation of turbulent flows on unstructured hybrid meshes. The cost of software errors is very high in largescale supercomputer simulations. Reproducing and localizing errors, especially ""magic"" unstable bugs related with wrong memory access, are extremely problematic due to the large amount of computing resources involved. In order to prevent, or at least notably filter out memory bugs, an approach of increased reliability is proposed for representing mesh data and organizing memory access. A set of containers is proposed, which causes no overhead in the release configuration compared to plain arrays. At the same time, it provides throughout access control in the safe mode configuration and additional compile-time protection from programming errors. Furthermore, it is fully compatible with heterogeneous computing within the OpenCL standard. The proposed approach provides internal debugging capabilities that allow us to localize problems directly in a supercomputer simulation."
10.14529/jsfi200201;Performance Reduction For Automatic Development of Parallel Applications  For Reconfigurable Computer Systems;In the paper, we review a suboptimal methodology of mapping of a task information graph on the architecture of a reconfigurable computer system. Using performance reduction methods, we can solve computational problems which need hardware costs exceeding the available hardware resource. We proved theorems, concerning properties of sequential reductions. In our case, we have the following types of reduction such as the reduction by number of basic subgraphs, by number of computing devices, and by data width. On the base of the proved theorems and corollaries, we developed the methodology of reduction transformations of a task information graph for its automatic adaptation to the architecture of a reconfigurable computer system. We estimated the maximum number of transformations, which, according to the suggested methodology, are needed for balanced reduction of the performance and hardware costs of applications for reconfigurable computer systems.
10.14529/jsfi200102;Development of Computational Pipeline Software for Genome/Exome Analysis on the K Computer;Pipeline software that comprise tool and application chains for specific data processing have found extensive utilization in the analysis of several data types, such as genome, in bioinformatics research. Recent trends in genome analysis require use of pipeline software for optimum utilization of computational resources, thereby facilitating efficient handling of large-scale biological data accumulated on a daily basis. However, use of pipeline software in bioinformatics tends to be problematic owing to their large memory and storage capacity requirements, increasing number of job submissions, and a wide range of software dependencies. This paper presents a massive parallel genome/exome analysis pipeline software that addresses these difficulties. Additionally, it can be executed on a large number of K computer nodes. The proposed pipeline incorporates workflow management functionality that performs effectively when considering the task-dependency graph of internal executions via extension of the dynamic task distribution framework. Performance results pertaining to the core pipeline functionality, obtained via evaluation experiments performed using an actual exome dataset, demonstrate good scalability when using over a thousand nodes. Additionally, this study proposes several approaches to resolve performance bottlenecks of a pipeline by considering the domain knowledge pertaining to internal pipeline executions as a major challenge facing pipeline parallelization. 
10.14529/jsfi200103;Supercomputing Technologies as Drive for Development of Enterprise Information Systems and Digital Economy;The article presents an analysis of approaches to the development of enterprise information systems that are in use today. One of the major trends that predetermines the agenda of information technology is the focus on parallel computing of large volumes of data using supercomputing technologies. The article considers the resulting ubiquitous move to distributed patterns of building enterprise information systems and avoiding monolithic architectures. The emphasis is placed on the importance of such fundamental characteristics of enterprise information systems as reliability, scalability, and maintainability. The article justifies the importance of machine learning in the context of effective big data analysis and competitive gain for business, vital for both maintaining a leading position in the market and surviving in conditions of global instability and digitalization of economy. Transition from storing the current state of a enterprise information system to storing a full log and history of all changes in the event stream is proposed as an instrument of achieving linearization of the data stream for subsequent parallel computing. There is a new view that is being shaped of specialists at the intersection of engineering and analytical disciplines, who would be able to effectively develop scalable systems and algorithms for data processing and integration of its results into company business processes.
10.14529/jsfi190404;Survey on Software Tools that Implement Deep Learning Algorithms on Intel/x86 and IBM/Power8/Power9 Platforms;Neural networks are becoming more and more popular in scientific field and in the industry. It is mostly because new solutions using neural networks show state-of-the-art results in the domains previously occupied by traditional methods, eg. computer vision, speech recognition etc. But to get these results neural networks become progressively more complex, thus needing a lot more training. The training of neural networks today can take weeks. This problems can be solved by parallelization of the neural networks training and using modern clusters and supercomputers, which can significantly reduce the learning time. Today, a faster training for data scientist is essential, because it allows to get the results faster to make the next decision.In this paper we provide an overview of distributed learning provided by the popular modern deep learning frameworks, both in terms of provided functionality and performance. We consider multiple hardware choices: training on multiple GPUs and multiple computing nodes.
10.14529/jsfi200104;Online MPI Process Mapping for Coordinating Locality and Memory Congestion on NUMA Systems;Mapping MPI processes to processor cores, called process mapping, is crucial to achieving the scalable performance on multi-core processors. By analyzing the communication behavior among MPI processes, process mapping can improve the communication locality, and thus reduce the overall communication cost. However, on modern non-uniform memory access (NUMA) systems, the memory congestion problem could degrade performance more severely than the locality problem because heavy congestion on shared caches and memory controllers could cause long latencies. Most of the existing work focus only on improving the locality or rely on offline profiling to analyze the communication behavior.We propose a process mapping method that dynamically performs the process mapping for adapting to communication behaviors while coordinating the locality and memory congestion. Our method works online during the execution of an MPI application. It does not require modifications to the application, previous knowledge of the communication behavior, or changes to the hardware and operating system. Experimental results show that our method can achieve performance and energy efficiency close to the best static mapping method with low overhead to the application execution. In experiments with the NAS parallel benchmarks on a NUMA system, the performance and total energy improvements are up to 34% (18.5% on average) and 28.9% (13.6% on average), respectively. In experiments with two GROMACS applications on a larger NUMA system, the average improvements in performance and total energy consumption are 21.6% and 12.6%, respectively.
10.14529/jsfi200101;State of the Art and Future Trends in Data Reduction for HighPerformance Computing;Research into data reduction techniques has gained popularity in recent years as storage capacity and performance become a growing concern. This survey paper provides an overview of leveraging points found in high-performance computing (HPC) systems and suitable mechanisms to reduce data volumes. We present the underlying theories and their application throughout the HPC stack and also discuss related hardware acceleration and reduction approaches. After introducing relevant use-cases, an overview of modern lossless and lossy compression algorithms and their respective usage at the application and file system layer is given. In anticipation of their increasing relevance for adaptive and in situ approaches, dimensionality reduction techniques are summarized with a focus on non-linear feature extraction. Adaptive approaches and in situ compression algorithms and frameworks follow. The key stages and new opportunities to deduplication are covered next. An unconventional but promising method is recomputation, which is proposed at last. We conclude the survey with an outlook on future developments.
10.14529/jsfi200202;Long Distance Geographically Distributed InfiniBand Based Computing;Collaboration between multiple computing centres, referred as federated computing is becoming important pillar of High Performance Computing (HPC) and will be one of its key components in the future. To test technical possibilities of future collaboration using 100Gb optic fiber link (Connection was 900 km in length with 9ms RTT time) we prepared two scenarios of operation. In the first one, Interdisciplinary Centre for Mathematical and Computational Modelling (ICM) in Warsaw and Centre of Informatics - Tricity Academic Supercomputer & networK (CI-TASK) in Gda?sk prepared a long distance geographically distributed computing cluster. System consisted of 14 nodes (10 nodes at ICM facility and 4 at TASK facility) connected using InfiniBand. Our tests demonstrate that it is possible to perform computationally intensive data analysis on systems of this class without substantial drop in performance for a certain type of workloads. Additionally, we show that it is feasible to use High Performance Parallex [1], high level abstraction libraries for distributed computing, to develop software for such geographically distributed computing resources and maintain desired efficiency. In the second scenario, we prepared distributed simulation-postprocessing-visualization workflow using ADIOS2 [2] and two programming languages (C++ and python). In this test we prove capabilities of performing different parts of analysis in seperate sites.
10.14529/jsfi200203;Potential of I/O Aware Workflows in Climate and Weather;The efficient, convenient, and robust execution of data-driven workflows and enhanced data management are essential for productivity in scientific computing. In HPC, the concerns of storage and computing are traditionally separated and optimised independently from each other and the needs of the end-to-end user. However, in complex workflows, this is becoming problematic. These problems are particularly acute in climate and weather workflows, which as well as becoming increasingly complex and exploiting deep storage hierarchies, can involve multiple data centres. The key contributions of this paper are: 1) A sketch of a vision for an integrated data-driven approach, with a discussion of the associated challenges and implications, and 2) An architecture and roadmap consistent with this vision that would allow a seamless integration into current climate and weather workflows as it utilises versions of existing tools (ESDM, Cylc, XIOS, and DDNs IME). The vision proposed here is built on the belief that workflows composed of data, computing, and communication-intensive tasks should drive interfaces and hardware configurations to better support the programming models. When delivered, this work will increase the opportunity for smarter scheduling of computing by considering storage in heterogeneous storage systems. We illustrate the performance-impact on an example workload using a model built on measured performance data using ESDM at DKRZ.
10.14529/jsfi200204;Bridging the Architecture Gap: Abstracting PerformanceRelevant Properties of Modern Server Processors;We propose several improvements to the execution-cache-memory (ECM) model, an analytic performance model for predicting single- and multicore runtime of steady-state loops on server processors. The model is made more general by strictly differentiating between application and machine models: an application model comprises the loop code, problem sizes, and other runtime parameters, while a machine model is an abstraction of all performance-relevant properties of a processor. Moreover, new first principles underlying the models estimates are derived from common microarchitectural features implemented by todays server processors to make the model more architecture independent, thereby extending its applicability beyond Intel processors. We introduce a generic method for determining machine models, and present results for relevant server-processor architectures by Intel, AMD, IBM, and Marvell/Cavium. Considering this wide range of architectures, the set of features required for adequate performance modeling is surprisingly small. To validate our approach, we compare performance predictions to empirical data for an OpenMP-parallel preconditioned CG algorithm, which includes compute- and memory-bound kernels. Both single- and multicore analysis shows that the model exhibits average and maximum relative errors of 5 % and 10 %. Deviations from the model and insights gained are discussed in detail.
10.14529/jsfi200105;Tools for GPU Computing - Debugging and Performance Analysis of Heterogenous HPC Applications;General purpose GPUs are now ubiquitous in high-end supercomputing. All but one (the Japanese Fugaku system, which is based on ARM processors) of the announced (pre-)exascale systems contain vast amounts of GPUs that deliver the majority of the performance of these systems. Thus, GPU programming will be a necessity for application developers using high-end HPC systems.However, programming GPUs efficiently is an even more daunting task than traditional HPC application development. This becomes even more apparent for large-scale systems containing thousands of GPUs. Orchestrating all the resources of such a system imposes a tremendous challenge to developers. Luckily a rich ecosystem of tools exist to assist developers in every development step of a GPU application at all scales.In this paper we present an overview of these tools and discuss their capabilities. We start with an overview of different GPU programming models, from low-level with CUDA over pragma-based models like OpenACC to high-level approaches like Kokkos. We discuss their respective tool interfaces as the main method for tools to obtain information on the execution of a kernel on the GPU. The main focus of this paper is on two classes of tools, debuggers and performance analysis tools. Debuggers help the developer to identify problems both on the CPU and GPU side as well as in the interplay of both. Once the application runs correctly, performance analysis tools can be used to pinpoint bottlenecks in the execution of the code and help to increase the overall performance.
10.14529/jsfi200205;Dawn: a Highlevel DomainSpecific Language Compiler Toolchain for Weather and Climate Applications;High-level programming languages that allow to express numerical methods and generate efficient parallel implementations are of key importance for the productivity of domain-scientists. The diversity and complexity of hardware architectures is imposing a huge challenge for large and complex models that must be ported and maintained for multiple architectures combining various parallel programming models. Several domain-specific languages (DSLs) have been developed to address the portability problem, but they usually impose a parallel model for specific numerical methods and support optimizations for limited scope operators. Dawn provides a high-level concise language for expressing numerical finite difference/volume methods using a sequential and descriptive language. The sequential statements are transformed into an efficient target-dependent parallel implementation by the Dawn compiler toolchain. We demonstrate our approach on the dynamical solver of the COSMO model, achieving performance improvements and code size reduction of up to 2x and 5x, respectively.
10.14529/jsfi200402;Enhancing the in Situ Visualization of Performance Data in Parallel CFD Applications;This paper continues the work initiated by the authors on the feasibility of using ParaView as visualization software for the analysis of parallel CFD codes performance. Current performance tools are unable to show their data on top of complex simulation geometries (e.g. an aircraft engine). In our previous paper, a plugin for the open-source performance tool Score-P has been introduced, which intercepts an arbitrary number of manually selected code regions (mostly functions) and send their respective measurements  amount of executions and cumulative time spent  to ParaView (through its in situ library, Catalyst), as if they are any other flow-related variable. This paper adds to such plugin the capacity to also show communication data (messages sent between MPI ranks) on top of the CFD mesh. Testing is done again with Rolls-Royces in-house CFD code, Hydra. The plugins original feature (regions measurements) is here revisited, in a bigger test-case, which is also used to illustrate the new feature (communication data). The benefits and overhead of the tool are discussed.
10.14529/jsfi200401;Effects of Using a Memory Stalled Core for Handling MPI Communication Overlapping in the SOR Solver on SXACE and SXAurora TSUBASA;Modern high-performance computing (HPC) systems consist of a large number of nodes featuring multi-core processors. Many computational fluid dynamics (CFD) codes utilize a Message Passing Interface (MPI) to exploit the potential of such systems. In general, the MPI communication costs increase as the number of MPI processes increases. In this paper, we discuss performance of the code in which a core is used as a dedicated communication core when the core cannot contribute to the performance improvement due to memory-bandwidth limitations. By using the dedicated communication core, the communication operations are overlapped with computation operations, thus enabling highly efficient computation by exploiting the limited memory bandwidth and idle cores. The performance evaluation shows that this code can hide the MPI communication times of 90% on the supercomputer SX-ACE system and 80% on the supercomputer SX-Aurora TSUBASA system, and the performance of the successive over-relaxation (SOR) method is improved by 32% on SX-ACE and 20% on SX-Aurora TSUBASA.
10.14529/jsfi200301;Accounting of Receptor Flexibility in UltraLarge Virtual Screens with VirtualFlow Using a Grey Wolf Optimization Method;Structure-based virtual screening approaches have the ability to dramatically reduce the time and costs associated to the discovery of new drug candidates. Studies have shown that the true hit rate of virtual screenings improves with the scale of the screened ligand libraries. Therefore, we have recently developed an open source drug discovery platform (VirtualFlow), which is able to routinely carry out ultra-large virtual screenings. One of the primary challenges of molecular docking is the circumstance when the protein is highly dynamic or when the structure of the protein cannot be captured by a static pose. To accommodate protein dynamics, we report the extension of VirtualFlow to allow the docking of ligands using a grey wolf optimization algorithm using the docking program GWOVina, which substantially improves the quality and efficiency of flexible receptor docking compared to AutoDock Vina. We demonstrate the linear scaling behavior of VirtualFlow utilizing GWOVina up to 128 000 CPUs. The newly supported docking method will be valuable for drug discovery projects in which protein dynamics and flexibility play a significant role.
10.14529/jsfi200302;Perspectives on Supercomputing and Artificial Intelligence Applications in Drug Discovery;This review starts with outlining how science and technology evaluated from last century into high throughput science and technology in modern era due to the Nobel-Prize-level inventions of combinatorial chemistry, polymerase chain reaction, and high-throughput screening. The evolution results in big data accumulated in life sciences and the fields of drug discovery. The big data demands for supercomputing in biology and medicine, although the computing complexity is still a grand challenge for sophisticated biosystems in drug design in this supercomputing era. In order to resolve the real-world issues, artificial intelligence algorithms (specifically machine learning approaches) were introduced, and have demonstrated the power in discovering structure-activity relations hidden in big biochemical data. Particularly, this review summarizes on how people modernize the conventional machine learning algorithms by combing non-numeric pattern recognition and deep learning algorithms, and successfully resolved drug design and high throughput screening issues. The review ends with the perspectives on computational opportunities and challenges in drug discovery by introducing new drug design principles and modeling the process of packing DNA with histones in micrometer scale space, a n example of how a macrocosm object gets into microcosm world.
10.14529/jsfi200303;Computational Modeling of the SARSCoV2 Main Protease Inhibition by the Covalent Binding of Prospective Drug Molecules;We illustrate modern modeling tools applied in the computational design of drugs acting as covalent inhibitors of enzymes. We take the Main protease (Mpro) from the SARS-CoV-2 virus as an important present-day representative. In this work, we construct a compound capable to block Mpro, which is composed of fragments of antimalarial drugs and covalent inhibitors of cysteine proteases. To characterize the mechanism of its interaction with the enzyme, the algorithms based on force fields, including molecular mechanics (MM), molecular dynamics (MD) and molecular docking, as well as quantum-based approaches, including quantum chemistry and quantum mechanics/molecular mechanics (QM/MM) methods, should be applied. The use of supercomputers is indispensably important at least in the latter approach. Its application to enzymes assumes that energies and forces in the active sites are computed using methods of quantum chemistry, whereas the rest of protein matrix is described using conventional force fields. For the proposed compound, containing the benzoisothiazolone fragment and the substitute at the uracil ring, we show that it can form a stable covalently bound adduct with the target enzyme, and thus can be recommended for experimental trials.
10.14529/jsfi200304;Computational Characterization of the Substrate Activation in the Active Site of SARSCoV2 Main Protease;Molecular dynamics simulations with the QM(DFT)/MM potentials are utilized to discriminate between reactive and nonreactive complexes of the SARS-CoV-2 main protease and its substrates. Classification of frames along the molecular dynamic trajectories is utilized by analysis of the 2D maps of the Laplacian of electron density. Those are calculated in the plane formed by the carbonyl group of the substrate and a nucleophilic sulfur atom of the cysteine residue that initiates enzymatic reaction. Utilization of the GPU-based DFT code allows fast and accurate simulations with the hybrid functional PBE0 and double-zeta basis set. Exclusion of the polarization functions accelerates the calculations 2-fold, however this does not describe the substrate activation. Larger basis set with d-functions on heavy atoms and p-functions on hydrogen atoms enables to disclose equilibrium between the reactive and nonreactive species along the MD trajectory. The suggested approach can be utilized to choose covalent inhibitors that will readily interact with the catalytic residue of the selected enzyme.
10.14529/jsfi200305;In Search of Noncovalent Inhibitors of SARSCoV2 Main Protease: Computer Aided Drug Design Using Docking and Quantum Chemistry;Two stages virtual screening of a database containing several thousand low molecular weight organic compounds is performed with the goal to find inhibitors of SARS-CoV-2 main protease. Overall near 41000 different 3D molecular structures have been generated from the initial molecules taking into account several conformers of most molecules. At the first stage the classical SOL docking program is used to determine most promising candidates to become inhibitors. SOL employs the MMFF94 force field, the genetic algorithm (GA) of the global energy optimization, takes into account the desolvation effect arising upon protein-ligand binding and the internal stress energy of the ligand. Parameters of GA are selected to perform the meticulous global optimization, and for docking of one ligand several hours on one computing core are needed on the average. The main protease model is constructed on the base of the protein structure from the Protein Data Bank complex 6W63. More than 1000 ligands structures have been selected for further postprocessing. The SOL score values of these ligands are  more negative than the threshold of -6.3 kcal/mol obtained for the native X77 ligand docking. Subsequent calculation of the protein-ligand binding enthalpy by the PM7 quantum-chemical semiempirical method with COSMO solvent model have narrowed down the number of best candidates. Finally, the diverse set of 20 most perspective candidates for the in vitro validation are selected.
10.14529/jsfi200306;Computational Approaches To Identify A Hidden Pharmacological Potential In Large Chemical Libraries;"To improve the discovery of more effective and less toxic pharmaceutical agents, large virtual repositories of synthesizable molecules have been generated to increase the explored chemical-pharmacological space diversity. Such libraries include billions of structural formulae of drug-like molecules associated with data on synthetic schemes, required building blocks, estimated physical-chemical parameters, etc. Clearly, such repositories are ""Big Data"". Thus, to identify the most promising compounds with the required pharmacological properties (hits) among billions of available opportunities, special computational methods are necessary. We have proposed using a combined computational approach, which combines structural similarity assessment, machine learning, and molecular modeling. Our approach has been validated in a project aimed at finding new pharmaceutical agents against HIV/AIDS and associated comorbidities from the Synthetically Accessible Virtual Inventory (SAVI), a 1.75 billion compound database. Potential inhibitors of HIV-1 protease and reverse transcriptase and agonists of toll-like receptors and STING, affecting innate immunity, were computationally identified. The activity of the three synthesized compounds has been confirmed in a cell-based assay. These compounds belong to the chemical classes, in which the agonistic effect on TLR 7/8 had not been previously shown. Synthesis and biological testing of several dozens of compounds with predicted antiretroviral activity are currently taking place at the NCI/NIH. We also carried out virtual screening among one billion substances to find compounds potentially possessing anti-SARS-CoV-2 activity. The selected hits' information has been accepted by the European Initiative ""JEDI Grand Challenge against COVID-19"" for synthesis and further biological evaluation. The possibilities and limitations of the approach are discussed."
10.14529/jsfi200403;Improving Quantum Annealing Performance on Embedded Problems;Recently, many researchers are investigating quantum annealing as a solver for real-world combinatorial optimization problems. However, due to the format of problems that quantum annealing solves and the structure of the physical annealer, these problems often require additional setup prior to solving. We study how these setup steps affect performance and provide insight into the interplay among them using the job-shop scheduling problem for our evaluation. We show that the empirical probability of success is highly sensitive to problem setup and that excess variables and large embeddings reduce performance. We then show that certain problem instances are unable to be solved without the use of additional post-processing methods. Finally, we investigate the effect of pausing during the anneal. Our results show that pausing within a certain time window can improve the probability of success, which is consistent with other work. However, we also show that the performance improvement due to pausing can be masked depending on properties of the embedding, and thus, special care must be taken for embedded problems.
10.14529/jsfi200406;Computer Design of Structure of Molecules of HighEnergy Tetrazines Calculation of Thermochemical Properties;The article presents high-performance calculations, using quantum chemical ab initio methods, of thermochemical characteristics of high-energy compounds: C2N6O4, C2N6O5, C2N6O6, C2H2N6O4, C3HN7O6, C3HN7O4F2, C4N10O12, C3HN6O4F, C4N10O8F4, C4N8O8F2. The IR absorption spectra, structural parameters and atomic displacements for the most intense vibrations, as well as the enthalpies of formation are provided in the article. The calculations were performed at the B3LYP/6-311+G(2d,p) level and using the combined methods CBS-4M and G4 within the Gaussian 09 application package (Linda paralellization). It is shown that the enthalpy of formation depends on the molecule structure.
10.14529/jsfi200405;Update on Performance Analysis of Different Computational Architectures: Molecular Dynamics in Application to ProteinProtein Interactions;Molecular dynamics has proved itself as a powerful computer simulation method to study dynamics, conformational changes, and interactions of biological macromolecules and their complexes. In order to achieve the best performance and efficiency, it is crucial to benchmark various hardware platforms for the simulations of realistic biomolecular systems with different size and timescale. Here, we compare performance and scalability of a number of commercially available computing architectures using all-atom and coarse-grained molecular dynamics simulations of water and the Ndc80-microtubule protein complex in the GROMACS-2019.4 package. We report typical single-node performance of various combinations of modern CPUs and GPUs, as well as multiple-node performance of the Lomonosov-2 supercomputer. These data can be used as the practical guidelines for choosing optimal hardware for molecular dynamics simulations.
10.14529/jsfi200404;Developing an Architectureindependent Graph Framework for Modern Vector Processors and NVIDIA GPUs;This paper describes the first-in-the-world attempt to develop an architectural-independent graph framework named VGL, designed for different modern architectures with high-bandwidth memory. Currently VGL supports two classes of architectures: NEC SX-Aurora TSUBASA vector processors and NVIDIA GPUs. However, VGL can be easily extended to other architectures due to its flexible software structure. VGL is designed to provide users with the possibility of selecting the most suitable architecture for solving a specific graph problem on a given input data, which, in return, allows to significantly outperform existing frameworks and libraries, developed for modern multicore CPUs and NVIDIA GPUs. Since VGL uses an identical set of computational and data abstractions for all architectures, its users can easily port graph algorithms between different target architectures without any source code modifications. Additionally, in this paper we show how graph algorithms should be implemented and optimised for NVIDIA GPU and NEC SX-Aurora TSUBASA architectures, demonstrating that both architectures have multiple similar properties and hardware features.
10.14529/jsfi210104;Computational Resource Consumption in Convolutional Neural Network Training - A Focus on Memory;Deep neural networks (DNNs) have grown in popularity in recent years thanks to the increase in computing power and the size and relevance of data sets. This has made it possible to build more complex models and include more areas of research and application. At the same time, the amount of data generated during the training process of these models puts great pressure on the capacity and bandwidth of the memory subsystem and, as a direct consequence, has become one of the biggest bottlenecks for the scalability of neural networks. Therefore, the optimizing of the workloads produced by DNNs in the memory subsystem requires a detailed understanding of access to the memory and the interactions between the processor, accelerator devices, and the system memory hierarchy. However, contrary to what would be expected, most DNN profilers work at a high level, so they only perform an analysis of the model and individual layers of the network leaving aside the complex interactions between all the hardware components involved in the training. This article shows the characterization performed using a convolutional neural network implemented in the two most popular frameworks: TensorFlow and Pytorch. Likewise, the behavior of the component interactions is discussed by varying the batch size for two sets of synthetic data and showing the results obtained by the profiler created for the study. Moreover, the results obtained when evaluating the AlexNet version on TensorFlow and its similarity in behavior when using a basic CNN are included.
10.14529/jsfi210102;Forecastability Measures that Describe the Complexity of a Site for Deep Learning Wind Predictions;The application of deep learning to wind time series for multi-step prediction obtains good results at short horizons. The accuracy of a wind forecast is highly dependent on the specific structure of wind in the specific location, as many local features influence wind behaviour. The characterization of the complexity of a site for wind prediction is defined as forecastability or predictability and can be obtained from the inner structure of the meteorological time series observations from a site. We analyze the time series structure searching for properties that have a high correlation with the prediction result, properties that can create measures that have the potential to describe the forecastability of a site. The best measures will show a high correlation with the accuracy of the predictions. In this work, we analyze wind time series from 126,692 wind locations in the US, where we apply several deep learning methods first, and then we verify several forecastability descriptors with the accuracy deep learning results. We require High-Performance Computing (HPC) resources for this task as the deep learning algorithms have sensible resource requirements and are applied to a large set of data. The measures defined and explored in this work are based on several techniques that decompose or transform the wind time-series. By combining several of these measures, we can obtain better predictors of the site complexity, which will allow us to evaluate the future error of a prediction on this site. Forecastability measures can contribute to a wind site multi-dimensional description, becoming a valuable tool for wind resource analysts and wind forecasters.
10.14529/jsfi210103;Size & Shape Matters: The Need of HPC Benchmarks of High Resolution Image Training for Deep Learning;One of the purposes of HPC benchmarks is to identify limitations and bottlenecks in hardware. This functionality is particularly influential when assessing performance on emerging tasks, the nature and requirements of which may not yet be fully understood. In this setting, a proper benchmark can steer the design of next generation hardware by properly identifying said requirements, and quicken the deployment of novel solutions. With the increasing popularity of deep learning workloads, benchmarks for this family of tasks have been gaining popularity. Particularly for image based tasks, which rely on the most well established family of deep learning models: Convolutional Neural Networks. Significantly, most benchmarks for CNN use low-resolution and fixed-shape (LR&FS) images. While this sort of inputs have been very successful for certain purposes, they are insufficient for some domains of special interest (e.g., medical image diagnosis or autonomous driving) where one requires higher resolutions and variable-shape (HR&VS) images to avoid loss of information and deformation. As of today, it is still unclear how does image resolution and shape variability affect the nature of the problem from a computational perspective. In this paper we assess the differences between training with LR&FS and HR&VS, as means to justify the importance of building benchmarks specific for the latter. Our results on three different HPC clusters show significant variations in time, resources and memory management, highlighting the differences between LR&FS and HR&VS image deep learning.
10.14529/jsfi210106;MicroWorkflows Data Stream Processing Model for Industrial Internet of Things;The fog computing paradigm has become prominent in stream processing for IoT systems where cloud computing struggles from high latency challenges. It enables the deployment of computational resources between the edge and cloud layers and helps to resolve constraints, primarily due to the need to react in real-time to state changes, improve the locality of data storage, and overcome external communication channels limitations. There is an urgent need for tools and platforms to model, implement, manage, and monitor complex fog computing workflows. Traditional scientific workflow management systems (SWMSs) provide modularity and flexibility to design, execute, and monitor complex computational workflows used in smart industry applications. However, they are mainly focused on batch execution of jobs consisting of tightly coupled tasks. Integrating data streams into SWMSs of IoT systems is challenging. We proposed a microworkflow model to redesign the monolith architecture of workflow systems into a set of smaller and independent workflows that support stream processing. Micro-workflow is an independent data stream processing service that can be deployed on different layers of the fog computing environment. To validate the feasibility and practicability of the micro-workflow refactoring, we provide intensive experimental analysis evaluating the interval between sensor messages, the time interval required to create a message, between sending sensor message and receiving the message in SWMS, including data serialization, network latency, etc. We show that the proposed decoupling support of the independence of implementation, execution, development, maintenance, and cross-platform deployment, where each micro-workflow becomes a standalone computational unit, is a suitable mechanism for IoT stream processing.
10.14529/jsfi210105;The MareNostrum Experimental Exascale Platform MEEP;Nascent Open Source Instruction Set Architectures such as OpenPOWER or RISC-V, allow software/hardware co-designers to fully utilize the underlying hardware, modify it or extend it based on their needs. In this paper, we introduce the vision of the MareNostrum Experimental Exascale Platform (MEEP), an Open Source platform enabling software and hardware stack experimentation  targeting the High-Performance Computing (HPC) ecosystem. MEEP is built with state-of-the-art FPGAs that support PCIe and High Bandwidth Memory (HBM), making it ideal to emulate chiplet-based HPC accelerators such as ACME, at the chip, package, and/or system level. MEEP provides an FPGA Shell containing standardized interfaces (I/O and memory), enabling an emulated accelerator to communicate with the hardware of the FPGA and ensures quick integration. The first demonstration of MEEP is mapping a new accelerator, the Accelerated Compute and Memory Engine (ACME), on to this digital laboratory. This enables exploration of this novel disaggregated architecture, which separates the computation from the memory operations, optimizing the accelerator for both dense (compute-bound) as well as sparse (memory-bandwidth bound) workloads. Dense workloads focus on the computational capabilities of the engine, while dedicated processors for memory accesses optimize non-unit stride and/or random memory accesses required by sparse workloads. MEEP is an open source digital laboratory that can provide a future environment for full-stack co-design and pre-silicon exploration.  MEEP invites software developers and hardware engineers to build the application, compiler, libraries and the hardware to solve future challenges in the HPC, AI, ML, and DL domains.
10.14529/jsfi210201;Accelerating Seismic Redatuming Using Tile LowRank Approximations on NEC SXAurora TSUBASA;With the aim of imaging subsurface discontinuities, seismic data recorded at the surface of the Earth must be numerically re-positioned inside the subsurface where reflections have originated, a process referred to as redatuming. The recently developed Marchenko method is able to handle full-wavefield data including multiple arrivals. A downside of this approach is that a multi-dimensional convolution operator must be repeatedly evaluated to solve an expensive inverse problem. As such an operator applies multiple dense matrix-vector multiplications (MVM), we identify and leverage the data sparsity structure for each frequency matrix and propose to accelerate the MVM step using tile low-rank (TLR) matrix approximations. We study the TLR impact on time-to-solution for the MVM using different accuracy thresholds whilst at the same time assessing the quality of the resulting subsurface seismic wavefields and show that TLR leads to a minimal degradation in terms of signal-to-noise ratio on a 3D synthetic dataset. We mitigate the load imbalance overhead and provide performance evaluation on two distributed-memory systems. Our MPI+OpenMP TLR-MVM implementation reaches up to 3X performance speedup against the dense MVM counterpart from NEC scientific library on 128 NEC SX-Aurora TSUBASA cards. Thanks to the second generation of high bandwidth memory technology, it further attains up to 67X performance speedup compared to the dense MVM from Intel MKL when running on 128 dual-socket 20-core Intel Cascade Lake nodes with DDR4 memory. This corresponds to 110 TB/s of aggregated sustained bandwidth for our TLR-MVM implementation, without suffering deterioration in the quality of the reconstructed seismic wavefields.
10.14529/jsfi210202;Porting and Optimizing Molecular Docking onto the SXAurora TSUBASA Vector Computer;In computer-aided drug design, the rapid identification of drugs is critical for combating diseases. A key method in this field is molecular docking, which aims to predict the interactions between two molecules. Molecular docking involves long simulations running compute-intensive algorithms, and thus, can profit a lot from hardware-based acceleration. In this work, we investigate the performance efficiency of the SX-Aurora TSUBASA vector computer for such simulations. Specifically, we present our methodology for porting and optimizing AutoDock, a widely-used molecular docking program. Using a number of platform-specific code optimizations, we achieved executions on the SX-Aurora TSUBASA that are in average 3.6 faster than on modern 128-core CPU servers, and up to a certain extent, competitive to V100 and A100 GPUs. To the best of our knowledge, this is the first molecular docking implementation for the SX-Aurora TSUBASA.
10.14529/jsfi210203;First Experience of Accelerating a FieldInduced Chiral Transition Simulation Using the SXAurora TSUBASA;An analysis method based on the Ginzburg-Landau equation for the superconductivity is applied to the field-induced chiral transition simulation (FICT). However, the FICT is time consuming because it takes approximately 10 hours on a single SX-ACE vector processor. Moreover, the FICT must be repeatedly performed with parameters changed to understand the mechanism of the phenomenon. The newly emerged SX-Aurora TSUBASA, the successor of the SX-ACE processor, is expected to provide much higher performance to the programs executed on the SX-ACE as is. However, the SX-Aurora TSUBASA processor has changed its architecture of compute nodes and gives users three different execution models, which leads to users' concerns and questions in terms of how three execution models should be selectively used. In this paper, we report the first experience of using the SX-Aurora TSUBASA processor for the FICT. Specifically, we have developed three implementations of the FICT corresponding to the three execution models suggested by the SX-Aurora TSUBASA. For acceleration of the FICT, improvement of the vectorization ratio in the program execution and the efficient transfer of data to the general purpose processor as the vector host from the vector processor as the vector engine is explored. The evaluation in this paper shows how acceleration of the FICT is achieved as well as how much effort of users is required.
10.14529/jsfi210204;Evaluating the Performance of OpenMP Offloading on the NEC SXAurora TSUBASA Vector Engine;The NEC SX-Aurora TSUBASA vector engine (VE) follows the tradition of long vector processors for high-performance computing (HPC). The technology combines the vector computing capabilities with the popularity of standard x86 architecture by integrating it as an accelerator. To decrease the burden of code porting for different accelerator types, the OpenMP specification is designed to be single parallel programming model for all of them. Besides the availability of compiler and runtime implementations, the functionality as well as the performance is important for the usability and acceptance of this paradigm. In this work, we present LLVM-based solutions for OpenMP target device offloading from the host to the vector engine and vice versa (reverse offloading). Therefore, we use our source-to-source transformation tool sotoc as well as the native LLVM-VE code path. We assess the functionality and present the first performance numbers of real-world HPC kernels. We discuss the advantages and disadvantage of the different approaches and show that our implementation is competitive to other GPU OpenMP runtime implementations. Our work gives scientific programmers new opportunities and flexibilities for the development of scalable OpenMP offloading applications for SX-Aurora TSUBASA.
